{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Boosting Techniques | Assignment"
      ],
      "metadata": {
        "id": "i4uYpyCymmLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n",
        "   - Boosting in machine learning is an ensemble technique that combines multiple weak learners to create a strong learner with improved predictive performance. A weak learner is a model that performs slightly better than random guessing, such as a shallow decision tree (often called a “stump”).\n",
        "\n",
        "     The core idea behind boosting is to train weak learners sequentially, where each new learner focuses on the mistakes made by the previous learners. During training, instances that are misclassified by earlier models are given higher weights, so subsequent learners pay more attention to these difficult cases. The predictions of all the learners are then combined using weighted voting or averaging, resulting in a final model that is much more accurate than any single weak learner.\n",
        "\n",
        "     Popular boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost. By emphasizing errors and iteratively correcting them, boosting reduces bias and variance, effectively turning weak learners into a powerful ensemble capable of handling complex patterns in data.\n",
        "\n",
        "     In short, boosting improves weak learners by making each one focus on the errors of the previous models, thereby progressively refining the overall prediction accuracy."
      ],
      "metadata": {
        "id": "MKcVSLBFmpon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "   - The primary difference between **AdaBoost** and **Gradient Boosting** lies in the way each successive model is trained to improve the ensemble. In **AdaBoost**, weak learners are trained sequentially, with each new model focusing more on the training instances that were misclassified by previous models. This is achieved by adjusting the weights of the training samples—misclassified samples receive higher weights, while correctly classified ones receive lower weights. The final prediction is obtained through a weighted vote of all learners. In contrast, **Gradient Boosting** trains each new model to predict the **residual errors** of the previous ensemble, effectively minimizing a chosen loss function using gradient descent. Instead of reweighting samples, Gradient Boosting directly fits the mistakes of the previous model, and the final output is the sum of all learners’ predictions, often scaled by a learning rate. Thus, AdaBoost emphasizes correcting misclassified samples, while Gradient Boosting focuses on reducing overall prediction error by learning residuals.\n"
      ],
      "metadata": {
        "id": "II1SsK-Em54T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. How does regularization help in XGBoost?\n",
        "   - In **XGBoost**, regularization helps prevent **overfitting** by penalizing the complexity of the model. Specifically, XGBoost incorporates both **L1 (lasso)** and **L2 (ridge)** regularization terms in its objective function, which constrain the weights of leaf nodes in the decision trees. By doing so, the model is discouraged from creating overly complex trees that fit the training data too closely, ensuring that it generalizes better to unseen data. Regularization also controls the depth and number of splits in trees, reduces variance, and improves the stability of predictions. Overall, it enables XGBoost to achieve high predictive accuracy while maintaining robustness and preventing overfitting, making it particularly effective on noisy or small datasets.\n"
      ],
      "metadata": {
        "id": "hsWfM4LBnJkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Why is CatBoost considered efficient for handling categorical data?\n",
        "   - **CatBoost** is considered efficient for handling categorical data because it **directly processes categorical features** without requiring manual preprocessing like one-hot encoding or label encoding, which can be memory-intensive and prone to overfitting. It uses a technique called **ordered target statistics**, where categorical values are converted into numerical representations based on the **average target value in a way that avoids data leakage**. Additionally, CatBoost employs **ordered boosting**, which ensures that the model only uses information from previous data points when calculating these statistics, further preventing overfitting. This approach allows CatBoost to efficiently handle high-cardinality categorical features, reduce preprocessing complexity, and achieve strong predictive performance on datasets with many categorical variables.\n"
      ],
      "metadata": {
        "id": "4Ezut6fFnYt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "   - Boosting techniques are preferred over bagging methods in real-world applications where the primary goal is to improve prediction accuracy by reducing bias, particularly when working with relatively clean datasets and simple base models. Boosting excels in scenarios that require sequential learning, where each model corrects the errors of the previous one, making it ideal for maximizing overall model performance despite a risk of overfitting in noisy datasets. In contrast, bagging is favored when dealing with high-variance, noisy data or parallel processing is a priority.\n",
        "\n",
        "Real-World Applications Favoring Boosting\n",
        "Boosting is commonly used in healthcare for tasks like breast cancer classification, where high accuracy is crucial and datasets are relatively clean.\n",
        "\n",
        "Financial services use boosting for risk prediction and fraud detection where incremental improvement in model accuracy impacts decision-making.\n",
        "\n",
        "Boosting is preferred in scenarios requiring fine-grained predictive performance such as customer churn prediction, marketing response modeling, and other classification or regression problems.\n",
        "\n",
        "Use of sklearn.datasets for Example Tasks\n",
        "For classification, breast cancer dataset (sklearn.datasets.load_breast_cancer()) is widely used to showcase boosting superiority in classification accuracy.\n",
        "\n",
        "For regression tasks, boosting can be applied effectively on real estate price prediction datasets, such as the California housing dataset (sklearn.datasets.fetch_california_housing()), demonstrating its ability to reduce bias and increase prediction precision."
      ],
      "metadata": {
        "id": "me1PWGt6nisE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy\n",
        "\n",
        "   (Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "27-hp3Iaoxo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Initialize and train the AdaBoost Classifier\n",
        "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of AdaBoost Classifier: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REW8Ms6ipFn0",
        "outputId": "aec4ba55-84a7-48bf-b838-6c0a86d5df45"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of AdaBoost Classifier: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score\n",
        "\n",
        "   (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "EWBefJa7pOFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Initialize and train the Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score of Gradient Boosting Regressor: {r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlQ0e2rCpShw",
        "outputId": "2257fabe-9301-41a2-adec-28fdaeff113b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared score of Gradient Boosting Regressor: 0.8004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy\n",
        "\n",
        "   (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "SgsD1LqppZ_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Initialize XGBoost Classifier\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# 4. Define hyperparameter grid for learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# 5. Perform GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Print best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# 7. Evaluate the model on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKRuOJu3pmg3",
        "outputId": "764437b4-02c4-49e8-81b1-1ad7e882e04e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [07:50:35] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Test Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn\n",
        "\n",
        "   (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "lGBNjgmNpsI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Initialize and train CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=200, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 6. Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# 7. Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - CatBoost Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Oh4JqsN7p2od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "● Hyperparameter tuning strategy\n",
        "\n",
        "● Evaluation metrics you'd choose and why\n",
        "\n",
        "● How the business would benefit from your model\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "- 1. Data Preprocessing & Handling Missing/Categorical Values\n",
        "\n",
        "Handle missing values:\n",
        "\n",
        "For numeric features: fill with median or mean.\n",
        "\n",
        "For categorical features: fill with mode or “Unknown” category.\n",
        "\n",
        "Categorical features:\n",
        "\n",
        "Use CatBoost or XGBoost directly handle categorical features (CatBoost is particularly efficient).\n",
        "\n",
        "Imbalanced data:\n",
        "\n",
        "Use class_weights in the model or techniques like SMOTE to balance classes.\n",
        "\n",
        "Feature scaling:\n",
        "\n",
        "Not required for tree-based boosting models.\n",
        "\n",
        "2. Choice of Boosting Algorithm\n",
        "\n",
        "CatBoost is chosen because:\n",
        "\n",
        "Handles categorical variables efficiently.\n",
        "\n",
        "Robust to missing values.\n",
        "\n",
        "Performs well on tabular data without extensive preprocessing.\n",
        "\n",
        "3. Hyperparameter Tuning Strategy\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV to tune:\n",
        "\n",
        "learning_rate\n",
        "\n",
        "depth\n",
        "\n",
        "iterations\n",
        "\n",
        "l2_leaf_reg\n",
        "\n",
        "class_weights to address imbalance\n",
        "\n",
        "Start with a small grid, then refine based on cross-validation results.\n",
        "\n",
        "4. Evaluation Metrics\n",
        "\n",
        "ROC-AUC score: Measures model’s ability to distinguish classes.\n",
        "\n",
        "Precision, Recall, F1-score: Important due to class imbalance (avoiding false negatives is critical).\n",
        "\n",
        "Confusion Matrix: Visual inspection of predictions.\n",
        "\n",
        "5. Business Impact\n",
        "\n",
        "Predicting loan defaults accurately allows:\n",
        "\n",
        "Minimizing financial losses.\n",
        "\n",
        "Better risk-based loan pricing.\n",
        "\n",
        "Targeted monitoring for high-risk customers."
      ],
      "metadata": {
        "id": "mon284ldp_HR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load dataset (example)\n",
        "# df = pd.read_csv('loan_data.csv')\n",
        "# For illustration, let's create a sample dataframe\n",
        "df = pd.DataFrame({\n",
        "    'age':[25, 40, 35, None, 50],\n",
        "    'income':[50000, 80000, None, 60000, 90000],\n",
        "    'loan_amount':[20000, 30000, 25000, 20000, 40000],\n",
        "    'marital_status':['Single','Married','Married','Single', None],\n",
        "    'default':[0,1,0,0,1]\n",
        "})\n",
        "\n",
        "# 2. Handle missing values\n",
        "df['age'].fillna(df['age'].median(), inplace=True)\n",
        "df['income'].fillna(df['income'].median(), inplace=True)\n",
        "df['marital_status'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# 3. Split features and target\n",
        "X = df.drop('default', axis=1)\n",
        "y = df['default']\n",
        "\n",
        "# Identify categorical features\n",
        "categorical_features = ['marital_status']\n",
        "\n",
        "# 4. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 5. Initialize CatBoostClassifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    learning_rate=0.1,\n",
        "    depth=4,\n",
        "    eval_metric='AUC',\n",
        "    random_state=42,\n",
        "    verbose=0,\n",
        "    class_weights=[1,2]  # Handle imbalance\n",
        ")\n",
        "\n",
        "# 6. Train the model\n",
        "model.fit(X_train, y_train, cat_features=categorical_features, eval_set=(X_test, y_test), verbose=50)\n",
        "\n",
        "# 7. Predictions and evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "y_proba = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "# Metrics\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Default','Default'], yticklabels=['No Default','Default'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8T3YGiCqqgiD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}