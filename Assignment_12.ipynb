{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree | Assignment"
      ],
      "metadata": {
        "id": "Op368W19OyRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "   - A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. In the context of classification, it works like a flowchart structure where data is split at each node based on feature values to reach a decision (final class label).\n",
        "   \n",
        "    How it works in Classification:--  In classification, a Decision Tree starts with a root node, which represents the entire dataset and is split into subsets based on the most important feature. At each step, the algorithm performs splitting at decision nodes, where it selects the best feature to divide the data, usually determined using metrics like the Gini Index (to measure impurity) or Entropy/Information Gain (to measure reduction in uncertainty). These splits form branches, which represent possible outcomes of the decision at each node. Eventually, the process leads to leaf nodes (terminal nodes), where the final classification decision is made, and each leaf corresponds to a particular class label. This step-by-step structure allows the model to classify data by following the path from the root to a leaf based on feature values.\n",
        "\n"
      ],
      "metadata": {
        "id": "zYkOzprfO1io"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "   - In Decision Trees, Gini Impurity and Entropy are two commonly used measures of node impurity that help determine the best feature splits. Gini Impurity measures the probability of incorrectly classifying a randomly chosen sample if it were labeled according to the class distribution in that node. A Gini value of 0 means the node is pure (all samples belong to one class), while higher values indicate more mixed classes. Entropy, on the other hand, comes from information theory and measures the level of uncertainty or disorder in a node. A value of 0 means complete purity, while higher values indicate greater randomness, with the maximum reached when classes are evenly distributed. In practice, a Decision Tree algorithm evaluates different possible splits and chooses the one that results in the greatest reduction in impurity—either by minimizing Gini or maximizing the reduction in Entropy (Information Gain). Thus, both measures guide the tree in creating branches that move toward purer subsets, ultimately improving classification accuracy."
      ],
      "metadata": {
        "id": "htnPu9fnPr6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "   - In Decision Trees, pre-pruning (also called early stopping) involves halting the growth of the tree during construction by setting constraints such as maximum depth, minimum number of samples required to split a node, or minimum information gain needed for a split. This prevents the tree from becoming too complex and overfitting the training data. A practical advantage of pre-pruning is that it makes the model simpler and faster to train, which is especially useful when working with large datasets. Post-pruning, on the other hand, allows the tree to grow fully and then trims back branches that do not improve performance on validation data. This reduces overfitting by removing unnecessary complexity after observing the complete structure. A practical advantage of post-pruning is that it usually produces a more accurate and generalizable model since it evaluates the impact of branches before deciding to remove them."
      ],
      "metadata": {
        "id": "1JtOUvHTP4i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "   - Information Gain in Decision Trees is a measure of how much a feature helps in reducing uncertainty or impurity when splitting the data. It is calculated as the difference between the entropy of the parent node and the weighted sum of the entropies of the child nodes after the split. A higher Information Gain means the feature provides more useful information for separating the classes, leading to purer subsets. This is important because Decision Trees choose the feature with the highest Information Gain at each step, ensuring that the most informative and discriminative splits are made first, which improves classification accuracy and helps the tree generalize better."
      ],
      "metadata": {
        "id": "bycnKFwyQELt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "   - Decision Trees are versatile and can be applied to both classification and regression tasks, which is why they are often demonstrated using datasets like the Iris dataset and the Boston Housing dataset.\n",
        "\n",
        "    In the Iris dataset (classification), Decision Trees can classify flowers into species (Setosa, Versicolor, Virginica) based on features like petal length, petal width, sepal length, and sepal width. This shows how decision trees are effective in real-world classification problems such as medical diagnosis (e.g., disease identification), customer segmentation, or spam detection.\n",
        "\n",
        "    In the Boston Housing dataset (regression), Decision Trees can predict house prices based on features like crime rate, number of rooms, property tax rate, and access to highways. This demonstrates their use in real-world regression problems such as property valuation, sales forecasting, or predicting equipment failure in manufacturing.\n",
        "\n",
        "  Advantages:\n",
        "\n",
        "      * Easy to understand and interpret with tree visualizations.\n",
        "      * Handle both categorical and numerical features without much preprocessing.\n",
        "      * Can model both classification and regression tasks.\n",
        "      * Fast training and prediction compared to some complex models.\n",
        "\n",
        " Limitations:\n",
        "\n",
        "      * Tend to overfit, especially with deep trees (though pruning helps).  \n",
        "      * Unstable — small data changes can lead to very different trees.\n",
        "      * Not as accurate as ensemble methods (Random Forests, Gradient Boosting).\n",
        "      * Can be biased if data is imbalanced.\n",
        "\n",
        " So, with datasets like Iris, Decision Trees highlight their strength in classification tasks, and with Boston Housing, they highlight their ability to handle regression tasks — but in both cases, overfitting and instability remain key challenges."
      ],
      "metadata": {
        "id": "Vioors5TQTRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "\n",
        "   ●   Load the Iris Dataset\n",
        "  \n",
        "  ● Train a Decision Tree Classifier using the Gini criterion\n",
        "   \n",
        "   ● Print the model’s accuracy and feature importances\n",
        "\n",
        "   (Include your Python code and output in the code box below.)    "
      ],
      "metadata": {
        "id": "ZD1PQm_cRptf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier using Gini index\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Classifier (Gini Criterion)\")\n",
        "print(\"Accuracy on test data: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzJCesQiSFz2",
        "outputId": "e7e3b914-9474-451b-a0bd-509544717464"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier (Gini Criterion)\n",
            "Accuracy on test data: 100.00%\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "\n",
        "   ● Load the Iris Dataset\n",
        "\n",
        "   ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "\n",
        "   (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "eI9cgbcoQhjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Train fully-grown Decision Tree (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Classifier Comparison on Iris Dataset\")\n",
        "print(\"---------------------------------------------------\")\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited*100:.2f}%\")\n",
        "print(f\"Accuracy with fully-grown tree: {accuracy_full*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG1fT2f1SZnz",
        "outputId": "55daf1a3-9c46-47a6-f675-4a46ee172ca6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier Comparison on Iris Dataset\n",
            "---------------------------------------------------\n",
            "Accuracy with max_depth=3: 100.00%\n",
            "Accuracy with fully-grown tree: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  Write a Python program to:\n",
        "\n",
        "   ● Load the Boston Housing Dataset\n",
        "\n",
        "   ● Train a Decision Tree Regressor\n",
        "\n",
        "   ● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "    (Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "mRGNJa_PSdtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load Boston Housing dataset\n",
        "# Note: load_boston is deprecated in latest sklearn versions.\n",
        "# If unavailable, use fetch_california_housing() as a substitute.\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(criterion=\"squared_error\", random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Regressor on Boston Housing Dataset\")\n",
        "print(\"-------------------------------------------------\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(boston.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "dNYhmuOdSqjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "     ● Load the Iris Dataset  \n",
        "\n",
        "    ● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "     ● Print the best parameters and the resulting model accuracy\n",
        "     \n",
        "     (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "wl1euMBhTLWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 6]\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree and GridSearchCV\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model\n",
        "best_clf = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = best_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Hyperparameter Tuning with GridSearchCV (Iris Dataset)\")\n",
        "print(\"--------------------------------------------------------------------\")\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Best Cross-Validation Score: {grid_search.best_score_:.2f}\")\n",
        "print(f\"Accuracy on Test Data: {accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAjdQxjaTdSA",
        "outputId": "7718e8e9-db0c-4886-883a-792e0bf3d9b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Hyperparameter Tuning with GridSearchCV (Iris Dataset)\n",
            "--------------------------------------------------------------------\n",
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 6}\n",
            "Best Cross-Validation Score: 0.94\n",
            "Accuracy on Test Data: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "     ● Handle the missing values\n",
        "\n",
        "     ● Encode the categorical features\n",
        "\n",
        "     ● Train a Decision Tree model\n",
        "\n",
        "     ● Tune its hyperparameters\n",
        "\n",
        "     ● Evaluate its performance\n",
        "\n",
        "    And describe what business value this model could provide in the real-world\n",
        "setting\n",
        "\n",
        "     -   If I were working as a data scientist in a healthcare company aiming to predict whether a patient has a certain disease, I would first address the missing values by imputing numerical features with statistical measures such as the mean or median and categorical features with the most frequent value or by assigning a new “Unknown” category. Next, I would encode categorical features, using one-hot encoding for nominal variables like blood type and ordinal encoding for ordered features such as disease stage, ensuring that the model can process both numerical and categorical data. After preprocessing, I would train a Decision Tree classifier on the dataset, starting with a simple tree and then optimizing it. Since Decision Trees are prone to overfitting, I would tune hyperparameters such as max_depth, min_samples_split, and min_samples_leaf using GridSearchCV or RandomizedSearchCV with cross-validation to find the balance between model complexity and generalization. Once the best model is chosen, I would evaluate its performance on a test set using metrics beyond accuracy, such as precision, recall, F1-score, and ROC-AUC, since in healthcare reducing false negatives (missing a disease case) is critical. In the real-world setting, this model could provide significant business value by supporting doctors in early disease detection, reducing the burden of manual screening, improving patient outcomes with timely interventions, and optimizing healthcare resources by prioritizing high-risk patients, ultimately leading to both better care delivery and cost savings for the organization."
      ],
      "metadata": {
        "id": "gjSWAxfOTf8Z"
      }
    }
  ]
}