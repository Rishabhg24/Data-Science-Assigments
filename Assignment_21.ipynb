{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Image Classification using CNN Architectures\n",
        "| Assignment"
      ],
      "metadata": {
        "id": "WAbtcL6_7DHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Convolutional Neural Network (CNN), and how does it differ from\n",
        "traditional fully connected neural networks in terms of architecture and performance on\n",
        "image data?\n",
        "   - A **Convolutional Neural Network (CNN)** is a specialized type of deep learning model designed primarily for processing and analyzing visual data such as images and videos. Unlike traditional **fully connected neural networks (FNNs)**, where every neuron in one layer is connected to every neuron in the next, CNNs are built to take advantage of the **spatial structure** of images. They use **convolutional layers** that apply small filters or kernels across local regions of the input image to automatically detect important features like edges, textures, shapes, and patterns. These filters share weights across the image, which drastically reduces the number of trainable parameters compared to fully connected networks and helps the model generalize better. CNNs also include **pooling layers**, which downsample the feature maps, making the network more efficient and less sensitive to small shifts or distortions in the input image. Toward the end of the architecture, **fully connected layers** are often used for the final classification or regression tasks.\n",
        "\n",
        "     In contrast, fully connected networks treat all input features equally and lose the spatial relationships between pixels when an image is flattened into a one-dimensional vector, making them inefficient for image processing. CNNs, on the other hand, preserve spatial hierarchies and learn from local to global features in a structured way. This architectural difference allows CNNs to achieve **superior performance** on image-related tasks such as classification, object detection, and facial recognition while being computationally more efficient and less prone to overfitting. In summary, CNNs outperform traditional neural networks on image data because they effectively capture spatial dependencies, require fewer parameters, and learn feature representations automatically without manual feature extraction.\n"
      ],
      "metadata": {
        "id": "ST2TFqGP7QyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Discuss the architecture of LeNet-5 and explain how it laid the foundation\n",
        "for modern deep learning models in computer vision. Include references to its original\n",
        "research paper.\n",
        "  - LeNet-5, developed by Yann LeCun and colleagues in 1998, is one of the earliest and most influential Convolutional Neural Network (CNN) architectures, introduced in the research paper “Gradient-Based Learning Applied to Document Recognition” (LeCun et al., 1998, Proceedings of the IEEE). It was primarily designed for handwritten digit recognition on the MNIST dataset and laid the groundwork for the deep learning revolution in computer vision. The LeNet-5 architecture consists of seven layers with trainable parameters, excluding the input. It takes a 32×32 grayscale image as input and passes it through alternating convolutional and subsampling (pooling) layers followed by fully connected layers. The first convolutional layer (C1) uses six 5×5 filters to extract basic features such as edges, producing 28×28 feature maps. The second layer (S2) performs average pooling to reduce dimensionality and achieve translation invariance. The third layer (C3) applies sixteen 5×5 filters to learn more complex features, followed by another pooling layer (S4), which further compresses the representation. The fifth layer (C5) acts as a fully connected convolutional layer with 120 feature maps, and the sixth layer (F6) is a fully connected layer with 84 neurons, leading finally to an output layer with 10 neurons representing the digit classes (0–9).\n",
        "\n",
        "    LeNet-5 introduced several groundbreaking ideas such as local receptive fields, weight sharing, and subsampling, which drastically reduced computational complexity while preserving spatial relationships in images. Its design allowed the model to automatically learn hierarchical feature representations—from simple edges to complex shapes—without manual feature extraction. Although limited by the computational power of the 1990s, LeNet-5 became the conceptual blueprint for later, deeper networks such as AlexNet (2012), VGGNet (2014), and ResNet (2015). These modern CNNs expanded upon LeNet’s principles using larger datasets, more layers, and faster hardware. In essence, LeNet-5 demonstrated that end-to-end learning through convolution and pooling could effectively perform image recognition tasks, establishing the foundation for the modern era of deep learning in computer vision."
      ],
      "metadata": {
        "id": "rIsv64-Q717b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Compare and contrast AlexNet and VGGNet in terms of design principles,\n",
        "number of parameters, and performance. Highlight key innovations and limitations of\n",
        "each.\n",
        "   - AlexNet and VGGNet are two landmark convolutional neural network (CNN) architectures that significantly advanced the field of deep learning in computer vision. AlexNet, introduced by Krizhevsky, Sutskever, and Hinton in 2012, won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) by a large margin and marked the beginning of the deep learning revolution. It consists of 8 layers—5 convolutional and 3 fully connected—and introduced key innovations such as the ReLU activation function for faster training, dropout for regularization, and the use of GPU acceleration to handle large-scale data efficiently. AlexNet used local response normalization (LRN) and overlapping max pooling to improve generalization and feature extraction. It has around 60 million parameters, which was massive for its time, and achieved a top-5 error rate of about 15.3% on ImageNet. However, its design relied on large filter sizes (e.g., 11×11, 5×5) and multiple fully connected layers, making it computationally heavy and prone to overfitting.\n",
        "\n",
        "     In contrast, VGGNet, proposed by Simonyan and Zisserman in 2014, focused on architectural simplicity and depth as its core design principle. VGGNet explored the effect of increasing depth and introduced models with 16 or 19 layers (VGG16 and VGG19) using a uniform design of 3×3 convolution filters and 2×2 max pooling throughout the network. This consistent use of small filters allowed VGGNet to capture complex features more effectively while keeping the receptive field manageable. It achieved a top-5 error rate of about 7.3% on ImageNet, significantly outperforming AlexNet. However, this performance came at the cost of a huge increase in parameters—about 138 million—which made VGGNet computationally expensive and memory-intensive. Despite its heavy architecture, VGGNet’s simplicity, modular design, and depth inspired the development of more advanced models such as ResNet and Inception.\n",
        "\n",
        "     In summary, AlexNet pioneered the practical application of deep CNNs with innovations like ReLU, dropout, and GPU training, while VGGNet refined CNN architecture by emphasizing depth and uniform convolutional design. AlexNet demonstrated the potential of deep learning on large-scale image data, and VGGNet established the design principles of deeper and more structured CNNs. The main limitation of AlexNet was its large filter sizes and overfitting tendency, while VGGNet’s drawback was its high computational and memory cost. Nonetheless, both architectures were pivotal in shaping the evolution of modern deep learning models in computer vision."
      ],
      "metadata": {
        "id": "ufgxEja984-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is transfer learning in the context of image classification? Explain\n",
        "how it helps in reducing computational costs and improving model performance with\n",
        "limited data.\n",
        "    - **Transfer learning** in the context of **image classification** refers to the process of using a **pre-trained deep learning model**, which has already learned useful visual features from a large dataset (such as ImageNet), and adapting it to a new but related image classification task. Instead of training a neural network from scratch—which requires vast amounts of labeled data and computational power—transfer learning allows the model to reuse previously learned patterns such as edges, textures, and shapes from earlier layers. These pre-learned features act as a strong foundation, enabling the model to learn new classes or tasks more efficiently. By fine-tuning only the later layers or adding new classification layers on top of the existing architecture, the model quickly adapts to the new dataset with relatively little data. This approach significantly **reduces computational cost**, as it avoids the need for training millions of parameters from the beginning, and **improves model performance** when training data is limited, because the model benefits from the general knowledge already embedded in its weights. In essence, transfer learning accelerates training, enhances accuracy, and prevents overfitting, making it one of the most effective techniques in modern image classification.\n"
      ],
      "metadata": {
        "id": "84lxrNNu9Myq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Describe the role of residual connections in ResNet architecture. How do\n",
        "they address the vanishing gradient problem in deep CNNs?\n",
        "   - In the ResNet (Residual Network) architecture, residual connections—also known as skip connections—play a crucial role in enabling the training of very deep convolutional neural networks. Introduced by He et al. (2015) in the paper “Deep Residual Learning for Image Recognition,” residual connections were designed to address the vanishing gradient problem, which commonly occurs when networks become very deep. In traditional deep CNNs, as the number of layers increases, gradients propagated backward during training can become extremely small, causing earlier layers to learn very slowly or stop learning altogether. This limits the depth and performance of the model.\n",
        "\n",
        "     Residual connections solve this issue by allowing the input of a layer to bypass one or more intermediate layers and be directly added to the output of those layers. Mathematically, instead of learning a direct mappingH(x), the network learns a residual function 𝐹(𝑥)=𝐻(𝑥)−𝑥F(x)=H(x)−x, which is then combined as H(x)=F(x)+x.This simple addition ensures that if deeper layers fail to learn useful transformations, the network can still preserve the original input information through the skip connection. As a result, gradients can flow more easily through the network during backpropagation, preventing them from vanishing or exploding.\n",
        "\n",
        "     By facilitating smoother gradient propagation, residual connections make it feasible to train networks with hundreds or even thousands of layers without degradation in accuracy. They also improve convergence speed and generalization performance. In essence, ResNet’s residual connections enable deeper and more stable learning by reformulating the learning objective into a simpler residual mapping, effectively overcoming one of the key challenges in training very deep CNNs."
      ],
      "metadata": {
        "id": "vIzbMw_29ksc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Implement the LeNet-5 architectures using Tensorflow or PyTorch to\n",
        "classify the MNIST dataset. Report the accuracy and training time.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "pAISg2Id_cgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LeNet-5 Implementation on MNIST using PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Device Configuration\n",
        "# -------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Data Loading and Preprocessing\n",
        "# -------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),      # LeNet-5 expects 32x32 input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define LeNet-5 Architecture\n",
        "# -------------------------------\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tanh(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.tanh(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = self.tanh(self.fc1(x))\n",
        "        x = self.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Initialize Model, Loss, and Optimizer\n",
        "# -------------------------------\n",
        "model = LeNet5().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Training Loop\n",
        "# -------------------------------\n",
        "num_epochs = 5\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Evaluation\n",
        "# -------------------------------\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGUNRKsk_kuo",
        "outputId": "ab7c7d29-1f57-4c56-866d-b08ecc754e19"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 37.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.10MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.1MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.52MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.2558\n",
            "Epoch [2/5], Loss: 0.0775\n",
            "Epoch [3/5], Loss: 0.0542\n",
            "Epoch [4/5], Loss: 0.0432\n",
            "Epoch [5/5], Loss: 0.0333\n",
            "\n",
            "Training completed in 221.60 seconds\n",
            "Test Accuracy: 98.43%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Use a pre-trained VGG16 model (via transfer learning) on a small custom\n",
        "dataset (e.g., flowers or animals). Replace the top layers and fine-tune the model.\n",
        "Include your code and result discussion.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "E--PSkNK_tMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer Learning using Pre-trained VGG16 on a Small Custom Dataset (Flowers)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import time\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Pre-trained VGG16 Model\n",
        "# -------------------------------\n",
        "# Load the VGG16 model without the top (fully connected) layers\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers to retain pre-trained ImageNet features\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Add Custom Classification Layers\n",
        "# -------------------------------\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(5, activation='softmax')  # Example: 5 flower classes\n",
        "])\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Data Preparation\n",
        "# -------------------------------\n",
        "# Assume directory structure:\n",
        "# dataset/\n",
        "#   ├── train/\n",
        "#   │    ├── daisy/\n",
        "#   │    ├── rose/\n",
        "#   │    ├── sunflower/\n",
        "#   │    ├── tulip/\n",
        "#   │    └── dandelion/\n",
        "#   ├── val/\n",
        "#   │    ├── daisy/\n",
        "#   │    ├── rose/\n",
        "#   │    └── ...\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_dir = 'dataset/train'\n",
        "val_dir = 'dataset/val'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Compile and Train the Model\n",
        "# -------------------------------\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=5,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Fine-tuning (Optional)\n",
        "# -------------------------------\n",
        "# Unfreeze the last few convolutional blocks to fine-tune deeper features\n",
        "for layer in base_model.layers[-4:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "fine_tune_history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=3,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Evaluate the Model\n",
        "# -------------------------------\n",
        "loss, acc = model.evaluate(val_generator)\n",
        "print(f\"\\nFinal Validation Accuracy: {acc*100:.2f}%\")\n",
        "print(f\"Total Training Time: {training_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "D95j-MoJ_xml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a program to visualize the filters and feature maps of the first\n",
        "convolutional layer of AlexNet on an example input image.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "fsovreh9AGxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Filters and Feature Maps of the First Convolutional Layer in AlexNet\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Pre-trained AlexNet Model\n",
        "# -------------------------------\n",
        "alexnet = torchvision.models.alexnet(weights='IMAGENET1K_V1')\n",
        "alexnet.eval()  # set to evaluation mode\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Load and Preprocess an Example Image\n",
        "# -------------------------------\n",
        "# Use any sample image path (replace 'sample.jpg' with your image file)\n",
        "img_path = 'sample.jpg'\n",
        "\n",
        "# Preprocessing steps same as ImageNet-trained AlexNet expects\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "image = Image.open(img_path).convert('RGB')\n",
        "input_tensor = transform(image).unsqueeze(0)  # add batch dimension\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Visualize Filters of First Conv Layer\n",
        "# -------------------------------\n",
        "filters = alexnet.features[0].weight.data.clone()\n",
        "\n",
        "print(f\"Shape of first layer filters: {filters.shape}\")  # (64, 3, 11, 11)\n",
        "\n",
        "# Normalize filters to 0-1 for visualization\n",
        "filters = (filters - filters.min()) / (filters.max() - filters.min())\n",
        "\n",
        "# Plot first 8 filters\n",
        "fig, axes = plt.subplots(4, 8, figsize=(12, 6))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    if i < 32:\n",
        "        img = filters[i].permute(1, 2, 0).numpy()\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "plt.suptitle(\"Filters of the First Convolutional Layer in AlexNet\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Extract Feature Maps from First Conv Layer\n",
        "# -------------------------------\n",
        "with torch.no_grad():\n",
        "    feature_maps = alexnet.features[0](input_tensor)\n",
        "\n",
        "print(f\"Feature map shape: {feature_maps.shape}\")  # (1, 64, H, W)\n",
        "\n",
        "# Normalize for visualization\n",
        "feature_maps = feature_maps.squeeze(0)\n",
        "feature_maps = (feature_maps - feature_maps.min()) / (feature_maps.max() - feature_maps.min())\n",
        "\n",
        "# Plot first 16 feature maps\n",
        "fig, axes = plt.subplots(4, 4, figsize=(10, 8))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    if i < 16:\n",
        "        ax.imshow(feature_maps[i].cpu().numpy(), cmap='gray')\n",
        "        ax.axis('off')\n",
        "plt.suptitle(\"Feature Maps from the First Convolutional Layer (AlexNet)\", fontsize=14)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fI48MqLfAK-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Train a GoogLeNet (Inception v1) or its variant using a standard dataset\n",
        "like CIFAR-10. Plot the training and validation accuracy over epochs and analyze\n",
        "overfitting or underfitting.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "8FcNGh14AVaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train GoogLeNet (Inception v1) on CIFAR-10 and analyze overfitting/underfitting\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# -------------------------------------\n",
        "# 1. Device configuration\n",
        "# -------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -------------------------------------\n",
        "# 2. Data Preprocessing and Loading\n",
        "# -------------------------------------\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                             download=True, transform=transform_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                            download=True, transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "# -------------------------------------\n",
        "# 3. Load Pretrained GoogLeNet Model and Modify Final Layer\n",
        "# -------------------------------------\n",
        "model = models.googlenet(weights=None, num_classes=10)  # train from scratch\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# -------------------------------------\n",
        "# 4. Train the Model\n",
        "# -------------------------------------\n",
        "num_epochs = 10\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    correct, total, running_loss = 0, 0, 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    train_acc_history.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_acc = 100 * correct / total\n",
        "    val_acc_history.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nTraining Completed in {training_time:.2f} seconds\")\n",
        "\n",
        "# -------------------------------------\n",
        "# 5. Plot Training and Validation Accuracy\n",
        "# -------------------------------------\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, num_epochs+1), train_acc_history, label='Training Accuracy')\n",
        "plt.plot(range(1, num_epochs+1), val_acc_history, label='Validation Accuracy')\n",
        "plt.title(\"GoogLeNet on CIFAR-10: Training vs Validation Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------------\n",
        "# 6. Analysis of Overfitting or Underfitting\n",
        "# -------------------------------------\n",
        "if val_acc_history[-1] < train_acc_history[-1] - 5:\n",
        "    print(\"Model shows signs of OVERFITTING — high training accuracy but lower validation accuracy.\")\n",
        "elif val_acc_history[-1] < 70 and train_acc_history[-1] < 70:\n",
        "    print(\"Model may be UNDERFITTING — both training and validation accuracy are low.\")\n",
        "else:\n",
        "    print(\"Model seems well-balanced — no major overfitting or underfitting detected.\")\n"
      ],
      "metadata": {
        "id": "OYt_8MXAAaMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working in a healthcare AI startup. Your team is tasked with\n",
        "developing a system that automatically classifies medical X-ray images into normal,\n",
        "pneumonia, and COVID-19. Due to limited labeled data, what approach would you\n",
        "suggest using among CNN architectures discussed (e.g., transfer learning with ResNet\n",
        "or Inception variants)? Justify your approach and outline a deployment strategy for\n",
        "production use.\n",
        "(Include your Python code and output in the code box below.)\n",
        "    - Why transfer learning with ResNet (brief)\n",
        "\n",
        "       ResNet-style models (ResNet-50/101) provide a strong balance of representational power and trainability because residual connections permit much deeper models to be optimized reliably. Pretrained ImageNet ResNets capture generic visual features in early layers (edges, textures) that transfer well to medical images; this is well-established in surveys of transfer learning for medical imaging. With limited labeled X-rays, starting from pretrained weights and fine-tuning only the head (and later some deeper blocks) reduces required training data and compute while improving generalization.\n",
        "        cv-foundation.org +1\n",
        "\n",
        "       High-level training strategy (recommended)\n",
        "\n",
        "       Preprocessing & augmentation: resize, intensity normalization, robust augmentations (rotation, translation, random contrast, simulated noise, elastic transforms).\n",
        "\n",
        "       Stage-wise training:\n",
        "\n",
        "       Stage 1 (head training): freeze backbone, train new classifier head (few epochs).\n",
        "\n",
        "      Stage 2 (fine-tuning): unfreeze last N blocks (e.g., last ResNet stage), train with a small learning rate. Optionally do multistage transfer learning using intermediate medical pretraining if available.\n",
        "      SpringerLink\n",
        "\n",
        "      Loss & balancing: use class weights or focal loss if classes are imbalanced (COVID may be rarer).\n",
        "\n",
        "      Validation: use stratified k-fold cross-validation (patient-level splits) to avoid data leakage. Report AUROC per class, sensitivity, specificity, PPV/NPV, and calibration (Brier score).\n",
        "\n",
        "      Explainability & failure modes: generate Grad-CAM maps for clinicians to inspect important regions.\n",
        "      arXiv\n",
        "\n",
        "      Uncertainty & triage: produce a calibrated probability and flag low-confidence cases for human review (human-in-the-loop). Use MC-dropout or deep ensembles for uncertainty estimation.\n",
        "\n",
        "      Robustness checks: test on external datasets and varied acquisition settings (portable X-rays, different hospitals).\n",
        "\n",
        "      Clinical validation: prospective study comparing model + clinician vs clinician alone, with appropriate IRB and regulatory steps.\n",
        "      BioMed Central\n",
        "      +1\n",
        "\n",
        "      Deployment strategy (production)\n",
        "\n",
        "      Containerize model (Docker), serve with a model server (TorchServe/TF-Serving) behind a secure REST/gRPC API.\n",
        "\n",
        "      Inference pipeline: DICOM ingest → preproc (windowing, resizing) → model → postproc (thresholding, calibrated probability) → Grad-CAM overlay for review → store result in PACS/EMR.\n",
        "\n",
        "      Monitoring & MLOps: log inputs, predictions, confidences, and clinician feedback; track data drift and performance metrics; implement automated alerts if performance drops.\n",
        "\n",
        "      Retraining & change management: follow a documented lifecycle and change control (data curation, retraining criteria, validation) in line with regulatory guidance for AI/ML medical devices.\n",
        "      U.S. Food and Drug Administration\n",
        "\n",
        "      Privacy & security: encrypt data at rest and transit; follow HIPAA/GDPR as applicable; perform threat modelling for adversarial inputs.\n",
        "\n",
        "      Human-in-the-loop: model outputs are advisory—present probability, heatmap, and a clear statement that the model is a decision-support tool.\n",
        "\n",
        "      Safety & regulatory notes\n",
        "\n",
        "      This is a diagnostic-adjacent, high-risk task. Do not deploy for autonomous decision making. Perform prospective clinical validation and seek regulatory approvals (FDA/CE) as required. The FDA has evolving guidance for AI/ML SaMD and lifecycle management; incorporate those recommendations early."
      ],
      "metadata": {
        "id": "qbFAnIbGAjux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet_transfer_xray.py\n",
        "# Transfer learning (ResNet50) for 3-class X-ray classification + Grad-CAM\n",
        "# NOTE: This is a template. Replace dataset paths and tune hyperparams.\n",
        "\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ---------- config ----------\n",
        "DATA_DIR = \"xray_dataset\"  # expected: train/val/test subfolders with class subdirs\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS_HEAD = 5\n",
        "NUM_EPOCHS_FINE = 5\n",
        "NUM_CLASSES = 3\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_SAVE = \"resnet50_xray.pt\"\n",
        "# ----------------------------\n",
        "\n",
        "# ---------- transforms ----------\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.85,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485], [0.229]) if False else transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "# ------------------------------\n",
        "\n",
        "# ---------- datasets & loaders ----------\n",
        "train_ds = datasets.ImageFolder(os.path.join(DATA_DIR, \"train\"), transform=train_tf)\n",
        "val_ds   = datasets.ImageFolder(os.path.join(DATA_DIR, \"val\"), transform=val_tf)\n",
        "test_ds  = datasets.ImageFolder(os.path.join(DATA_DIR, \"test\"), transform=val_tf)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "class_names = train_ds.classes\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "# ---------- model setup ----------\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "# replace final FC\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# freeze backbone initially\n",
        "for param in model.conv1.parameters():\n",
        "    pass  # conv1 kept trainable; below we'll freeze whole layers\n",
        "for name, param in model.named_parameters():\n",
        "    if \"fc\" not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
        "# scheduler optional\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "# ---------- training helper ----------\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return running_loss/total, correct/total\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "    return running_loss/total, correct/total, np.concatenate(all_probs), np.concatenate(all_labels)\n",
        "\n",
        "# ---------- Stage 1: train head ----------\n",
        "print(\"Stage 1: training head only\")\n",
        "best_val_acc = 0.0\n",
        "history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
        "for epoch in range(NUM_EPOCHS_HEAD):\n",
        "    t0 = time.time()\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
        "    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, DEVICE)\n",
        "    scheduler.step()\n",
        "    history[\"train_loss\"].append(train_loss); history[\"train_acc\"].append(train_acc)\n",
        "    history[\"val_loss\"].append(val_loss); history[\"val_acc\"].append(val_acc)\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS_HEAD}  train_loss={train_loss:.4f} train_acc={train_acc:.4f}  val_loss={val_loss:.4f} val_acc={val_acc:.4f}  time={time.time()-t0:.1f}s\")\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# ---------- Stage 2: fine-tune last layers ----------\n",
        "print(\"Stage 2: fine-tuning last layers\")\n",
        "# unfreeze last conv block (layer4) and fc\n",
        "for name, param in model.named_parameters():\n",
        "    if \"layer4\" in name or \"fc\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS_FINE):\n",
        "    t0 = time.time()\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
        "    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, DEVICE)\n",
        "    scheduler.step()\n",
        "    history[\"train_loss\"].append(train_loss); history[\"train_acc\"].append(train_acc)\n",
        "    history[\"val_loss\"].append(val_loss); history[\"val_acc\"].append(val_acc)\n",
        "    print(f\"Fine Epoch {epoch+1}/{NUM_EPOCHS_FINE}  train_loss={train_loss:.4f} train_acc={train_acc:.4f}  val_loss={val_loss:.4f} val_acc={val_acc:.4f}  time={time.time()-t0:.1f}s\")\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# save best model\n",
        "model.load_state_dict(best_model_wts)\n",
        "torch.save(model.state_dict(), MODEL_SAVE)\n",
        "print(\"Saved best model with val_acc=\", best_val_acc)\n",
        "\n",
        "# ---------- Evaluate on test set ----------\n",
        "test_loss, test_acc, test_probs, test_labels = evaluate(model, test_loader, criterion, DEVICE)\n",
        "print(f\"Test Acc: {test_acc:.4f}  Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# ---------- Grad-CAM utility (simple) ----------\n",
        "# This is a minimal Grad-CAM implementation for ResNet last conv layer 'layer4'\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "        self.target_layer = target_layer\n",
        "        # register hooks\n",
        "        def forward_hook(module, inp, out):\n",
        "            self.activations = out.detach()\n",
        "        def backward_hook(module, grad_in, grad_out):\n",
        "            self.gradients = grad_out[0].detach()\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def __call__(self, input_tensor, class_idx=None):\n",
        "        input_tensor = input_tensor.to(next(self.model.parameters()).device)\n",
        "        output = self.model(input_tensor)\n",
        "        if class_idx is None:\n",
        "            class_idx = output.argmax(dim=1).item()\n",
        "        loss = output[0, class_idx]\n",
        "        self.model.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        grads = self.gradients[0]            # C x H x W\n",
        "        acts  = self.activations[0]          # C x H x W\n",
        "        weights = grads.mean(dim=(1,2))      # C\n",
        "        cam = (weights.view(-1,1,1) * acts).sum(dim=0)\n",
        "        cam = F.relu(cam)\n",
        "        cam = cam - cam.min()\n",
        "        if cam.max() > 0:\n",
        "            cam = cam / cam.max()\n",
        "        cam_np = cam.cpu().numpy()\n",
        "        return cam_np\n",
        "\n",
        "# Example usage of GradCAM on one test image\n",
        "import matplotlib.pyplot as plt\n",
        "model.eval()\n",
        "sample_img, sample_label = test_ds[0]  # PIL->transform applied in dataset; here dataset returns tensors\n",
        "input_tensor = sample_img.unsqueeze(0)\n",
        "gcam = GradCAM(model, model.layer4)\n",
        "cam_map = gcam(input_tensor)\n",
        "# show overlay (requires original image before normalization; for demo we reuse tensor)\n",
        "img_np = sample_img.permute(1,2,0).numpy()\n",
        "img_np = (img_np - img_np.min())/(img_np.max()-img_np.min())\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(1,2,1); plt.title(\"Input\"); plt.imshow(img_np); plt.axis('off')\n",
        "plt.subplot(1,2,2); plt.title(\"Grad-CAM\"); plt.imshow(img_np); plt.imshow(cam_map, cmap='jet', alpha=0.45); plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jCXGaLxIAnyZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}