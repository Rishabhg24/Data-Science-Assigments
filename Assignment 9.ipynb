{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment Questions"
      ],
      "metadata": {
        "id": "BMSJF-NrZtA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "   - In machine learning, a parameter refers to a configuration variable that the learning algorithm learns from the training data. These parameters determine the model's predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "BDnognOBZw_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation?\n",
        "  - Correlation is a statistical measure that describes the relationship between two variables — specifically, how they move in relation to each other.\n",
        "\n",
        "  What does negative correlation mean?\n",
        "   - A negative correlation means that as one variable increases, the other decreases — they move in opposite directions.\n",
        "\n"
      ],
      "metadata": {
        "id": "zsN_he5naEy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "   - Machine Learning is a branch of artificial intelligence (AI) that focuses on developing systems that can learn from data, identify patterns, and make decisions with minimal human intervention.\n",
        "   \n",
        "        Main Components of  Machine Learning:\n",
        "    + Data\n",
        "    + model\n",
        "    + Algorithm\n",
        "    + Features\n",
        "    + Labels (Target)\n",
        "    + Loss Function (or Cost Function)\n",
        "    + Training\n",
        "    + Evaluation\n",
        "    + Hyperparameters"
      ],
      "metadata": {
        "id": "id6w9BJ_aTQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        "   - The loss value helps determine whether a machine learning model is good by measuring how accurately the model predicts the target output. It represents the difference between the predicted values and the actual values — the smaller this difference, the better the model is performing. During training, the model continuously adjusts its parameters to minimize the loss, aiming to make more accurate predictions. A low loss value indicates that the model's predictions are close to the actual data, which is generally a sign of a well-performing model. However, it's important to evaluate the loss not just on training data but also on validation or test data to ensure the model is not overfitting — meaning it works well on training data but poorly on unseen data. Therefore, monitoring the loss value over time and across different data sets helps determine whether the model is learning effectively and generalizing well, which are key indicators of a \"good\" model."
      ],
      "metadata": {
        "id": "9ugyOkYGcbKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "   - Continuous Variables:\n",
        "   \n",
        "     * These are numeric variables that can take any value within a range\n",
        "     * They are often the result of measurement.\n",
        "     * Can have decimals or fractions.\n",
        "      \n",
        "      Examples:\n",
        "                Height (e.g., 170.5 cm)\n",
        "                Weight (e.g., 65.2 kg)\n",
        "                Temperature (e.g., 22.3°C)\n",
        "                Age (in years or even days)\n",
        "\n",
        "    Continuous variables have infinite possible values within a range.\n",
        "\n",
        "  Categorical Variables:\n",
        "  \n",
        "  * These are variables that represent categories or groups.\n",
        "  * They are often the result of classification.\n",
        "  * Can be text labels or coded as numbers (e.g., 0 = Male, 1 = Female).\n",
        "\n",
        "   Types:\n",
        "          Nominal – No natural order\n",
        "             Example: Gender, Color, City\n",
        "\n",
        "           Ordinal – Has a meaningful order\n",
        "             Example: Education level (High School < Bachelor < Master)\n",
        "\n",
        "   Examples:\n",
        "   \n",
        "   Marital status (Single, Married, Divorced)\n",
        "   Product category (Electronics, Clothing, Furniture)\n",
        "   Survey rating (Poor, Fair, Good, Excellent)"
      ],
      "metadata": {
        "id": "hPM-vQi1c5CL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "   - In machine learning, handling categorical variables is crucial because most algorithms require numerical input. To convert categorical data into a format suitable for modeling, several encoding techniques are commonly used. One of the most basic methods is label encoding, which assigns a unique numeric value to each category. This is useful for ordinal variables where the order of categories matters (e.g., low, medium, high). However, for nominal variables (where no order exists), one-hot encoding is more appropriate—it creates separate binary columns for each category, allowing the model to treat them independently. Another method is ordinal encoding, where ordered categories are manually mapped to numerical values based on their rank. For datasets with many unique categories, frequency encoding (replacing each category with how often it appears) or target encoding (replacing categories with the average value of the target variable for that category) can be used, although target encoding must be applied carefully to avoid data leakage. The choice of encoding technique depends on the type of categorical variable, the machine learning algorithm used, and the size and complexity of the data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gECCBOvYewWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "   - Training and testing a dataset are two fundamental steps in building and evaluating a machine learning model.\n",
        "\n",
        "Training a dataset means using a portion of your data to teach the model how to recognize patterns or relationships between the input features and the target output. During this phase, the model learns by adjusting its internal parameters to minimize errors on the training data.\n",
        "\n",
        "Testing a dataset, on the other hand, involves using a separate portion of data that the model has never seen before to evaluate how well it has learned. This step checks the model’s ability to generalize and make accurate predictions on new, unseen data. By comparing the model’s predictions on the test data with the actual outcomes, you can assess its performance and identify issues like overfitting or underfitting.\n",
        "\n",
        "Together, training and testing ensure that the model not only fits the data it was trained on but can also perform well in real-world scenarios.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cGmRydiaf7cy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "   - sklearn.preprocessing is a module in the scikit-learn Python library that provides a variety of tools and functions to prepare and transform data before feeding it into a machine learning model.\n",
        "\n"
      ],
      "metadata": {
        "id": "xyXbckQqgegj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?\n",
        "   - A test set is a portion of a dataset reserved exclusively for evaluating the performance of a trained machine learning model. After a model is trained on the training set, it is tested on this unseen test set to see how well it generalizes to new, real-world data. The test set helps provide an unbiased estimate of the model’s accuracy, error, or other performance metrics. Using a separate test set is crucial to ensure that the model hasn't simply memorized the training data but can actually make reliable predictions on data it has never encountered before."
      ],
      "metadata": {
        "id": "8QAprZ-WgwnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "    - In Python, data is typically split into training and testing sets using functions like train_test_split from the sklearn.model_selection module. This function randomly divides the dataset, usually into a common ratio such as 70-30 or 80-20, where the larger portion is used for training the model and the smaller portion is reserved for testing its performance. The training set is used to teach the model patterns in the data, while the test set evaluates how well the model generalizes to new, unseen data. This split is crucial to avoid overfitting and to get a realistic measure of model accuracy.\n",
        "\n",
        "     When approaching a machine learning problem, the process typically starts with understanding the problem and the data—defining the objective and gathering relevant datasets. Next, the data is cleaned and preprocessed, handling missing values and encoding categorical variables. Then, the data is split into training and testing sets. After that, an appropriate model is selected and trained on the training data by adjusting its parameters. The model is then evaluated on the test data using relevant metrics. Based on the results, the model may be fine-tuned, retrained, or different algorithms tried. Finally, once a satisfactory model is obtained, it can be deployed for real-world predictions while continuously monitored for performance. This structured approach helps ensure the model is both accurate and reliable."
      ],
      "metadata": {
        "id": "bNM8UmiRg7hz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "    - We perform Exploratory Data Analysis (EDA) before fitting a model because it helps us understand the data deeply, identify patterns, spot anomalies, and uncover important relationships between variables. EDA allows us to check for missing values, outliers, or errors that could negatively impact the model’s performance. It also guides decisions on how to preprocess the data—for example, which features to transform, encode, or engineer. By visualizing distributions and correlations, we can select the most relevant features and detect potential issues like multicollinearity or imbalance in classes. Ultimately, EDA ensures that the data is clean, well-understood, and ready for modeling, which increases the chances of building an accurate and robust machine learning model."
      ],
      "metadata": {
        "id": "1MaWZtQLhQ4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "    - Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It tells us how one variable changes when the other variable changes. If both variables tend to increase or decrease together, they have a positive correlation. If one variable tends to increase while the other decreases, they have a negative correlation. The correlation is usually quantified by a correlation coefficient that ranges from -1 to 1 — where 1 means perfect positive correlation, -1 means perfect negative correlation, and 0 means no linear relationship between the variables. Correlation helps us understand whether and how strongly variables are related, but it does not imply causation."
      ],
      "metadata": {
        "id": "9smdmCAdhfCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "    - A negative correlation means that two variables move in opposite directions: when one variable increases, the other tends to decrease, and vice versa. In other words, they have an inverse relationship. For example, if you look at the relationship between the number of hours spent watching TV and test scores, you might find a negative correlation—more TV time could be associated with lower test scores. The strength of this relationship is measured by a correlation coefficient ranging from 0 to -1, where values closer to -1 indicate a stronger negative correlation. However, like all correlations, a negative correlation doesn’t imply that one variable causes the other to change; it only indicates an association.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JuGzowGshp-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "    - To find the correlation between variables in Python, you can use the Pandas library, which provides a convenient .corr() method to calculate the correlation matrix for a dataset. This method computes the Pearson correlation coefficient by default, which measures the linear relationship between variables. By applying .corr() to a DataFrame, you get a matrix showing correlation values between every pair of variables. If you want to find the correlation between just two specific variables, you can use the .corr() function on the two columns directly. Additionally, Pandas allows you to specify other correlation methods like Spearman or Kendall if you need to capture non-linear relationships. This approach makes it easy to quickly explore how variables relate to each other before further analysis or modeling."
      ],
      "metadata": {
        "id": "oZy0268lh3_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "    - Causation refers to a relationship where one event or variable directly causes a change in another. In contrast, correlation indicates that two variables are related or move together, but it does not mean that one causes the other. The key difference is that causation implies a cause-and-effect relationship, while correlation only shows an association without proving causality. For example, there might be a correlation between ice cream sales and drowning incidents because both increase during summer, but buying ice cream does not cause drowning. Instead, the warmer weather causes both more ice cream consumption and more swimming, which can unfortunately lead to more drowning incidents. Understanding this distinction is important to avoid misinterpreting data and drawing incorrect conclusions."
      ],
      "metadata": {
        "id": "kZu7EI6XiUtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example\n",
        "    - An optimizer in machine learning is an algorithm used to adjust a model’s parameters, such as weights and biases, in order to minimize the loss function during training and improve the model’s accuracy. Different types of optimizers achieve this in various ways. The basic Gradient Descent optimizer updates parameters using the entire dataset to move in the direction that reduces error. Stochastic Gradient Descent (SGD), on the other hand, updates parameters using one data point at a time, making learning faster but noisier. To balance these, Mini-batch Gradient Descent processes small batches of data for more stable and efficient updates. Enhancements like Momentum accelerate learning by adding a fraction of the previous update to the current one, helping the model converge faster and avoid getting stuck in local minima. The popular Adam optimizer combines momentum and adaptive learning rates for each parameter, making it very effective and widely used in training complex models like deep neural networks. Each optimizer offers different trade-offs between speed, stability, and accuracy depending on the problem and dataset."
      ],
      "metadata": {
        "id": "KVbBvwd2i0Sy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?\n",
        "    - sklearn.linear_model is a module in the scikit-learn Python library that provides a variety of linear models for regression and classification tasks. These models assume a linear relationship between input features and the target variable, making them simple yet powerful tools for many problems. The module includes algorithms like Linear Regression, Logistic Regression, Ridge Regression, Lasso Regression, and others. Each model in sklearn.linear_model offers different techniques for fitting linear relationships, handling regularization to prevent overfitting, and working with classification or regression targets. It’s widely used because of its ease of use, efficiency, and strong theoretical foundation in statistics and machine learning."
      ],
      "metadata": {
        "id": "qMMb0ppZjH1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        "    - The method model.fit() in machine learning is used to train or fit the model on a given dataset. When you call fit(), the model learns the patterns and relationships from the input data by adjusting its internal parameters (like weights in linear regression or coefficients in decision trees) to best map the inputs to the target outputs.\n",
        "\n",
        "     What arguments must be given?\n",
        "       \n",
        "       X: The input features — typically a 2D array or DataFrame where each row is a sample and each column is a feature.\n",
        "\n",
        "       y: The target values or labels — usually a 1D array or Series corresponding to the output for each sample.\n",
        "\n"
      ],
      "metadata": {
        "id": "g-8YRM7FjcLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n",
        "    - The method model.predict() is used to make predictions using a trained machine learning model. After the model has been trained with fit(), calling predict() on new input data generates the model’s output, such as predicted labels for classification or predicted values for regression.\n",
        "\n",
        "     What arguments must be given?\n",
        "      X: The input features for which you want to make predictions. This should be a 2D array or DataFrame where each row represents a sample and each column represents a feature."
      ],
      "metadata": {
        "id": "96gchjEHjvQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?\n",
        "    - Continuous variables are numeric variables that can take any value within a range, including decimals. They represent measurements and can be infinitely precise, such as height, weight, temperature, or age.\n",
        "\n",
        "Categorical variables represent distinct groups or categories that don’t have a numeric meaning or natural order (nominal), or they may have a meaningful order (ordinal). Examples include gender, color, country (nominal), or education level, customer satisfaction rating (ordinal).\n",
        "\n",
        "In short, continuous variables are measured on a continuous scale, while categorical variables classify data into separate categories.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aK1Hk7amkKva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "    - Feature scaling is the process of normalizing or standardizing the range of independent variables (features) in a dataset so that they have a similar scale. Since different features can have vastly different units or magnitudes (like age in years vs. income in thousands), scaling ensures that no single feature dominates the learning process just because of its scale.\n",
        "\n",
        "     How does feature scaling help in machine learning?\n",
        "      \n",
        "       * Feature scaling helps machine learning models perform better and faster in several ways:\n",
        "           * Improves convergence speed: Algorithms like gradient descent converge more quickly when features are on a similar scale\n",
        "           * Prevents bias: Models that rely on distance calculations (e.g., k-nearest neighbors, SVMs) or regularization (e.g., Ridge, Lasso) work better when features contribute equally.\n",
        "           *Enhances accuracy: Scaling helps prevent some features from disproportionately influencing the model due to their larger numeric ranges.\n",
        "           * Required for certain algorithms: Some models, like Principal Component Analysis (PCA) or neural networks, assume data is scaled to function correctly.\n",
        "\n"
      ],
      "metadata": {
        "id": "x2cqe-7pkZHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n",
        "    - In Python, feature scaling is typically performed using the preprocessing tools provided by the scikit-learn library. Two common techniques are standardization and min-max scaling. Standardization can be done using the StandardScaler class, which transforms features to have a mean of zero and a standard deviation of one, helping many algorithms perform better. Min-max scaling is done with the MinMaxScaler, which rescales features to a fixed range, usually between 0 and 1. Both scalers are applied by first fitting them to the data using the fit() method to learn scaling parameters, and then transforming the data with transform(). Often, this is combined into a single step with fit_transform() when working with training data. Importantly, when applying scaling in a machine learning pipeline, you fit the scaler only on the training set and then apply the learned transformation to the test set to prevent data leakage. These simple and efficient methods make feature scaling straightforward in Python."
      ],
      "metadata": {
        "id": "B5FXOmRfk6Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "    - sklearn.preprocessing is a module in the scikit-learn Python library that provides a wide range of tools and utilities for preprocessing and transforming data before feeding it into machine learning models. This module includes functions and classes to scale features, encode categorical variables, normalize data, handle missing values, generate polynomial features, and more. By standardizing or normalizing data, encoding categories into numbers, or transforming features, sklearn.preprocessing helps improve the performance and accuracy of many machine learning algorithms, ensuring the data is in the right format and scale for effective training."
      ],
      "metadata": {
        "id": "WynpSlsWlMMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "    - In Python, data is commonly split into training and testing sets using the train_test_split function from the sklearn.model_selection module. This function randomly divides the dataset into two parts: one for training the model and the other for testing its performance on unseen data. Typically, you specify the proportion of data to include in the test set—for example, 20% for testing and 80% for training—using the test_size parameter. You pass in your features (X) and target labels (y), and train_test_split returns four subsets: X_train, X_test, y_train, and y_test. Splitting the data this way helps ensure that the model learns patterns from the training set and is evaluated fairly on data it hasn’t seen before, providing a more realistic measure of how it will perform in real-world scenarios."
      ],
      "metadata": {
        "id": "letSkFGelVFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "    - Data encoding is the process of transforming data, especially categorical variables, into a numerical format that machine learning algorithms can understand and work with. Since most algorithms require numeric input, encoding converts categories like colors, labels, or text into numbers."
      ],
      "metadata": {
        "id": "erG6gOXblegJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSOq9FxjZoR8"
      },
      "outputs": [],
      "source": []
    }
  ]
}