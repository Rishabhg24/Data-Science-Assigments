{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Data Collection\n",
        "\n",
        "        * Download the dataset from the provided Google Drive link.\n",
        "\n",
        "        * Verify file formats (CSV recommended).\n",
        "\n",
        "        * Combine multiple CSVs if needed for 2016–2017 data."
      ],
      "metadata": {
        "id": "T1BCNhDL90yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Example: load all CSVs in a folder\n",
        "path = \"path_to_downloaded_folder/*.csv\"\n",
        "all_files = glob.glob(path)\n",
        "\n",
        "df_list = [pd.read_csv(file) for file in all_files]\n",
        "data = pd.concat(df_list, ignore_index=True)\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "WMbWR27q-BT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Data Preprocessing\n",
        "\n",
        "       * Handle missing values: drop or impute with mean/median.\n",
        "\n",
        "       * Ensure consistency: check duplicates, correct datatypes.\n",
        "\n",
        "       * Normalize/scale features (especially price and volume).\n",
        "\n",
        "       * Convert timestamps to datetime."
      ],
      "metadata": {
        "id": "6BNCMGB5-Jag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values\n",
        "data = data.fillna(method='ffill')  # forward fill missing values\n",
        "\n",
        "# Convert timestamp\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "# Normalize numerical features\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "numerical_cols = ['Open', 'Close', 'High', 'Low', 'Volume']\n",
        "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n"
      ],
      "metadata": {
        "id": "vHOZHTrl-doT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Exploratory Data Analysis (EDA)\n",
        "\n",
        "         * Visualize trends in price, volume, and liquidity.\n",
        "\n",
        "         * Compute correlations to detect influential factors."
      ],
      "metadata": {
        "id": "0k5NRu2B-njs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Trend of closing price\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(data['Date'], data['Close'])\n",
        "plt.title('Cryptocurrency Closing Price Trend')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Normalized Price')\n",
        "plt.show()\n",
        "\n",
        "# Correlation heatmap\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(data.corr(), annot=True, cmap='coolwarm')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-1ZdK5jC-sTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Feature Engineering\n",
        "\n",
        "         * Liquidity-related features:\n",
        "\n",
        "         * Moving averages (MA): short-term & long-term.\n",
        "\n",
        "         * Volatility: standard deviation over rolling window.\n",
        "\n",
        "         * Liquidity ratio: Volume / Price."
      ],
      "metadata": {
        "id": "PkDnM_1s-u8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Moving averages\n",
        "data['MA_7'] = data['Close'].rolling(window=7).mean()\n",
        "data['MA_30'] = data['Close'].rolling(window=30).mean()\n",
        "\n",
        "# Volatility\n",
        "data['Volatility'] = data['Close'].rolling(window=7).std()\n",
        "\n",
        "# Liquidity ratio\n",
        "data['Liquidity_Ratio'] = data['Volume'] / data['Close']\n",
        "\n",
        "# Fill NaN from rolling calculations\n",
        "data.fillna(method='bfill', inplace=True)\n"
      ],
      "metadata": {
        "id": "NqOysUZh-7Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Model Selection\n",
        "\n",
        "     Since this is time-series regression, options:\n",
        "\n",
        "      * ML Models: Random Forest, XGBoost, Linear Regression.\n",
        "\n",
        "      * Deep Learning: LSTM or simple MLP for sequence-based predictions.\n",
        "\n",
        "      * Here, I’ll demonstrate a Multilayer Perceptron (MLP) example:"
      ],
      "metadata": {
        "id": "I1cajRPr--rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Features and target\n",
        "features = ['Open', 'High', 'Low', 'Close', 'Volume', 'MA_7', 'MA_30', 'Volatility', 'Liquidity_Ratio']\n",
        "target = 'Liquidity_Ratio'\n",
        "\n",
        "X = data[features].values\n",
        "y = data[target].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Build MLP model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='linear')  # regression output\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n"
      ],
      "metadata": {
        "id": "OsMq79o7_PnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Model Evaluation"
      ],
      "metadata": {
        "id": "xvynrY8W_SFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"RMSE: {rmse:.4f}, R² Score: {r2:.4f}\")\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uW9d4PRx_VEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Hyperparameter Tuning\n",
        "\n",
        "       * Use GridSearchCV for classical ML models or Keras Tuner for deep learning.\n",
        "\n",
        "       * Tune: learning rate, number of neurons, batch size, and number of layers."
      ],
      "metadata": {
        "id": "b-EL1Z0e_XE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Local Deployment (Optional)\n",
        "\n",
        "Deploy using Streamlit or Flask to create a simple interface:\n"
      ],
      "metadata": {
        "id": "64T_fYqa_m62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example Streamlit code\n",
        "# Save this as app.py and run `streamlit run app.py`\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.load_model('mlp_liquidity_model.h5')\n",
        "\n",
        "st.title(\"Cryptocurrency Liquidity Prediction\")\n",
        "\n",
        "# User inputs\n",
        "open_price = st.number_input(\"Open Price\")\n",
        "high_price = st.number_input(\"High Price\")\n",
        "low_price = st.number_input(\"Low Price\")\n",
        "close_price = st.number_input(\"Close Price\")\n",
        "volume = st.number_input(\"Volume\")\n",
        "\n",
        "input_data = np.array([[open_price, high_price, low_price, close_price, volume, 0, 0, 0, 0]])  # add zeros for engineered features\n",
        "prediction = model.predict(input_data)\n",
        "st.write(f\"Predicted Liquidity Ratio: {prediction[0][0]:.4f}\")"
      ],
      "metadata": {
        "id": "vIfOVezt_cvq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}