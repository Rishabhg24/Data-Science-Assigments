{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression | Assignment"
      ],
      "metadata": {
        "id": "SWcMruANrbsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?\n",
        "  - Logistic Regression is a supervised machine learning algorithm used primarily for classification tasks, where the goal is to predict discrete outcomes such as whether an email is spam or not, or if a customer will make a purchase. Unlike Linear Regression, which predicts continuous numeric values using a linear relationship between input features and the target variable, Logistic Regression predicts the probability that a given input belongs to a particular category. It does this by applying the sigmoid (logistic) function to the linear combination of input features, effectively mapping the output to a range between 0 and 1. This probability is then used to classify the input into classes (e.g., 0 or 1). While Linear Regression uses mean squared error as its loss function, Logistic Regression uses log loss (or cross-entropy loss). The key difference lies in their purpose and output: Linear Regression is used for predicting quantities, whereas Logistic Regression is used for making categorical decisions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U1I0G1O2rrnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the role of the Sigmoid function in Logistic Regression.\n",
        "   - The sigmoid function in Logistic Regression is used to convert the linear output of the model into a probability value between 0 and 1. Logistic Regression works by computing a weighted sum of the input features (like in linear regression), but since the goal is to classify data into categories‚Äîtypically 0 or 1‚Äîwe need to interpret the output as a probability.The sigmoid function, defined as œÉ(z)= 1/(1+e)‚àíz, takes the linear result\n",
        "ùëß\n",
        "z and maps it to a value in the range [0, 1]. This output represents the probability that a given input belongs to the positive class (class 1). If the probability is greater than or equal to 0.5, the model usually classifies the input as class 1; otherwise, it is classified as class 0. Thus, the sigmoid function enables Logistic Regression to make meaningful and interpretable probabilistic predictions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1DuD4UXkr4CF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Regularization in Logistic Regression and why is it needed?\n",
        "   - Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty to the loss function based on the magnitude of the model's coefficients. In simple terms, it discourages the model from relying too heavily on any one feature by keeping the weights (coefficients) small.\n",
        "\n",
        "When a model becomes too complex‚Äîespecially if there are many features‚Äîit might fit the training data very well but perform poorly on new, unseen data. This is known as overfitting. Regularization addresses this problem by adding a term to the loss function that increases as the coefficients become larger.\n",
        "\n",
        "There are two common types of regularization used in Logistic Regression:\n",
        "\n",
        " - L1 Regularization (Lasso) ‚Äì Adds the absolute values of the coefficients to the loss function. It can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
        "\n",
        " - L2 Regularization (Ridge) ‚Äì Adds the square of the coefficients to the loss function. It penalizes large weights but does not usually shrink them to zero.\n",
        "\n",
        "Regularization is needed to improve the generalization ability of the model, ensuring it performs well not just on the training data but also on unseen data. It makes the model more stable and less sensitive to noise or irrelevant features."
      ],
      "metadata": {
        "id": "N-hc1dVauGuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are some common evaluation metrics for classification models, and\n",
        "why are they important?\n",
        "   - Some common evaluation metrics for classification models include accuracy, precision, recall, F1-score, and the ROC-AUC score. These metrics are essential for understanding how well a classification model performs, especially when dealing with imbalanced datasets or when the cost of different types of errors varies.\n",
        "\n",
        "    - Accuracy measures the overall correctness of the model by calculating the proportion of correctly predicted instances out of the total. While it's simple and useful, it can be misleading when classes are imbalanced.\n",
        "\n",
        "   - Precision is the ratio of true positive predictions to the total predicted positives. It‚Äôs important when the cost of false positives is high, such as in spam detection.\n",
        "\n",
        "   - Recall (or sensitivity) is the ratio of true positives to the actual positives. It‚Äôs crucial when missing a positive case is costly, like in disease detection.\n",
        "\n",
        "   - F1-score is the harmonic mean of precision and recall, offering a balance between the two. It is especially useful when you need to balance false positives and false negatives.\n",
        "\n",
        "   - ROC-AUC score measures the model's ability to distinguish between classes across all thresholds. A higher AUC indicates better performance.\n",
        "\n",
        "These metrics help in choosing the right model and in understanding the trade-offs involved in different types of errors, which is critical for making reliable predictions in real-world applications."
      ],
      "metadata": {
        "id": "V2ex5ZwNubIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. : Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "(Use Dataset from sklearn package)"
      ],
      "metadata": {
        "id": "2q8VvgVCuz8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split into features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Logistic Regression model: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fydQlD8lu_-d",
        "outputId": "e138798b-5fdd-4c8c-8cb8-cd66b912fe61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Logistic Regression model: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "E1JPHw_AvRKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model with L2 regularization (default)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "for feature, coef in zip(X.columns, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14ongWlRvV3G",
        "outputId": "da277cc4-9185-4e22-af77-0d41237fd882"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            "mean radius: 1.0274\n",
            "mean texture: 0.2215\n",
            "mean perimeter: -0.3621\n",
            "mean area: 0.0255\n",
            "mean smoothness: -0.1562\n",
            "mean compactness: -0.2377\n",
            "mean concavity: -0.5326\n",
            "mean concave points: -0.2837\n",
            "mean symmetry: -0.2267\n",
            "mean fractal dimension: -0.0365\n",
            "radius error: -0.0971\n",
            "texture error: 1.3706\n",
            "perimeter error: -0.1814\n",
            "area error: -0.0872\n",
            "smoothness error: -0.0225\n",
            "compactness error: 0.0474\n",
            "concavity error: -0.0429\n",
            "concave points error: -0.0324\n",
            "symmetry error: -0.0347\n",
            "fractal dimension error: 0.0116\n",
            "worst radius: 0.1117\n",
            "worst texture: -0.5089\n",
            "worst perimeter: -0.0156\n",
            "worst area: -0.0169\n",
            "worst smoothness: -0.3077\n",
            "worst compactness: -0.7727\n",
            "worst concavity: -1.4286\n",
            "worst concave points: -0.5109\n",
            "worst symmetry: -0.7469\n",
            "worst fractal dimension: -0.1009\n",
            "\n",
            "Model Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "p0AdI2eWvkHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Iris dataset (multiclass: 3 classes)\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model with multi_class='ovr'\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgyujALLvxRE",
        "outputId": "81d59980-e38b-47e9-8eef-063fbc3aff41"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pJrR6M-ssF4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. : Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "ExTijkKQvzUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # 'liblinear' supports both l1 and l2 penalties\n",
        "}\n",
        "\n",
        "# Initialize logistic regression\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and score\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(f\"Best Cross-Validated Accuracy: {grid.best_score_:.4f}\")\n",
        "\n",
        "# Test set accuracy\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xi9_n1Vv-C6",
        "outputId": "200d55b2-134a-49e9-f547-6db2a7a6afc0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Cross-Validated Accuracy: 0.9670\n",
            "Test Set Accuracy: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to standardize the features before training Logistic\n",
        "Regression and compare the model's accuracy with and without scaling.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n",
        ""
      ],
      "metadata": {
        "id": "jXQuRC2wwSh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=10000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression with scaling\n",
        "model_scaled = LogisticRegression(max_iter=10000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy without Scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with Scaling:    {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UghQr1cOwlmp",
        "outputId": "1b8df627-91df-4471-d9a1-efee813bc14c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.9561\n",
            "Accuracy with Scaling:    0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you‚Äôd take to build a\n",
        "Logistic Regression model ‚Äî including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case.\n",
        "   - To build a Logistic Regression model for predicting customer responses in an e-commerce marketing campaign with only 5% of customers responding, a careful and structured approach is essential due to the class imbalance. First, I would begin by exploring and preprocessing the data‚Äîthis includes handling missing values, encoding categorical features, engineering useful variables (like customer purchase history or engagement), and splitting the data into training and testing sets using stratified sampling to maintain the class distribution. Since Logistic Regression is sensitive to feature scale, I would standardize all numerical features using a method like StandardScaler to ensure they contribute equally to the model. To address the significant class imbalance, I would apply techniques such as setting class_weight='balanced' in the Logistic Regression model, which adjusts weights inversely proportional to class frequencies, or use oversampling methods like SMOTE to synthetically increase the minority class. For model tuning, I would use GridSearchCV to optimize hyperparameters such as the regularization strength C, the penalty type (l1 or l2), and the solver. When evaluating the model, I would avoid relying solely on accuracy and instead focus on metrics better suited to imbalanced data, such as precision, recall, F1-score, ROC-AUC, and the confusion matrix. Additionally, I would consider threshold tuning to adjust the decision boundary, aiming to maximize recall without incurring excessive false positives, aligning the model performance with the business goal of identifying as many potential responders as possible while managing marketing costs effectively."
      ],
      "metadata": {
        "id": "fQHEaZDNwvZw"
      }
    }
  ]
}