{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Learning | Assignment"
      ],
      "metadata": {
        "id": "FfNTUerLbzG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "   - Ensemble Learning in machine learning is a technique where multiple models, often referred to as \"learners\" or \"base models,\" are combined to solve the same problem with the goal of achieving better performance than any single model alone. The key idea behind ensemble learning is that by aggregating the predictions of several diverse models, the overall system can make more accurate and robust predictions. This approach leverages the strengths of individual models while minimizing their weaknesses, as different models may capture different patterns in the data or make different types of errors. By combining them—through methods like bagging, boosting, or stacking—the ensemble reduces errors related to bias and variance, leading to improved generalization on unseen data."
      ],
      "metadata": {
        "id": "YH0Secrzb47i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Bagging and Boosting?\n",
        "   - Bagging and Boosting are both ensemble learning techniques, but they differ in how they build and combine multiple models. Bagging (short for Bootstrap Aggregating) builds multiple models independently and in parallel using random subsets of the training data (with replacement). Each model is trained separately, and their outputs are typically combined through majority voting (for classification) or averaging (for regression), which helps reduce variance and prevent overfitting. In contrast, Boosting builds models sequentially, where each new model focuses on correcting the errors made by the previous ones. The models are dependent on each other, and weights are assigned to training instances so that misclassified ones get more attention in the next iteration. This sequential learning process aims to reduce both bias and variance, making Boosting generally more accurate but also more prone to overfitting if not properly tuned."
      ],
      "metadata": {
        "id": "ZvQh_8NPcOwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "  - Bootstrap sampling is a statistical technique where multiple random samples are drawn with replacement from a dataset. This means that each sample is the same size as the original dataset, but some data points may appear multiple times while others may not appear at all. In Bagging methods like Random Forest, bootstrap sampling plays a crucial role by creating diverse subsets of the training data for each individual model (or decision tree). Since each tree is trained on a different subset of data, it learns different patterns or makes different errors, introducing diversity among the models. This diversity is key to Bagging's effectiveness because when the predictions of multiple diverse models are combined—typically through majority voting—the ensemble can reduce variance, improve generalization, and become more robust to overfitting compared to a single model trained on the full dataset."
      ],
      "metadata": {
        "id": "FB0t8Ic_cZsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "   - Out-of-Bag (OOB) samples are the data points that are not included in a particular bootstrap sample during the training of ensemble models like Random Forests. Since bootstrap sampling is done with replacement, on average, about one-third of the original dataset is left out in each sample. These left-out instances are called OOB samples for that specific model. The OOB score is an internal validation method where each model in the ensemble is tested on its own OOB samples. The predictions from all models for their respective OOB samples are then aggregated to estimate the overall performance of the ensemble. This provides a reliable and efficient way to evaluate the model's accuracy without needing a separate validation set or cross-validation, making it especially useful when data is limited."
      ],
      "metadata": {
        "id": "7h4IuMWbcmBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "   - Feature importance analysis in a single Decision Tree is based on how much each feature contributes to reducing impurity (such as Gini impurity or entropy) at each split in the tree. The more a feature is used to make key decisions that result in large information gains, the higher its importance score. However, this analysis can be unstable and biased, especially if the tree is deep or overfitted, since small changes in the data can significantly affect which features are selected for splitting. In contrast, Random Forests, which are ensembles of many decision trees trained on different bootstrap samples and random subsets of features, provide a more reliable and robust measure of feature importance. By averaging the importance scores of each feature across all trees, Random Forests reduce the variance and bias of the individual trees' assessments. This leads to a more generalized and stable understanding of which features truly matter across the entire dataset."
      ],
      "metadata": {
        "id": "g1q_IMnSc5_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "    \n",
        "    ● Load the Breast Cancer dataset using sklearn.datasets load_breast_cancer()\n",
        "\n",
        "    ● Train a Random Forest Classifier\n",
        "    \n",
        "    ● Print the top 5 most important features based on feature importance scores.\n"
      ],
      "metadata": {
        "id": "bhmU4u0jdEy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort features by importance in descending order\n",
        "top_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_features.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d3H6SmtdgCy",
        "outputId": "5595d303-8937-4173-9b0f-db159f3b7cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "             Feature  Importance\n",
            "          worst area    0.139357\n",
            "worst concave points    0.132225\n",
            " mean concave points    0.107046\n",
            "        worst radius    0.082848\n",
            "     worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  Write a Python program to:\n",
        "\n",
        "    ● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "    ● Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "zMSpZgOIdj-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a single Decision Tree\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "tree_preds = tree.predict(X_test)\n",
        "tree_accuracy = accuracy_score(y_test, tree_preds)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_preds = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_preds)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy of Single Decision Tree: {tree_accuracy:.4f}\")\n",
        "print(f\"Accuracy of Bagging Classifier:   {bagging_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "05DR0di5dvHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "\n",
        "    ● Train a Random Forest Classifier\n",
        "\n",
        "    ● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "    ● Print the best parameters and final accuracy\n"
      ],
      "metadata": {
        "id": "Idxi5JKPd3bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define the model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [2, 4, 6, None]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred = best_model.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Final Accuracy on Test Set: {final_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EMNvq1ieAzI",
        "outputId": "ee69bb74-60f2-4d8c-9461-497e7f5b6c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 2, 'n_estimators': 10}\n",
            "Final Accuracy on Test Set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "\n",
        "    ● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "    Housing dataset\n",
        "    \n",
        "    ● Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "IrwwZvGpeInU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize regressors\n",
        "bagging = BaggingRegressor(random_state=42)\n",
        "random_forest = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Train regressors\n",
        "bagging.fit(X_train, y_train)\n",
        "random_forest.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_bagging = bagging.predict(X_test)\n",
        "y_pred_rf = random_forest.predict(X_test)\n",
        "\n",
        "# Calculate MSE\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print results\n",
        "print(f\"Bagging Regressor MSE: {mse_bagging:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZLMHA9dfUSZ",
        "outputId": "e047d5cd-0edf-4be2-a5f1-61172c1d7b32"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2824\n",
            "Random Forest Regressor MSE: 0.2554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "    ● Choose between Bagging or Boosting\n",
        "\n",
        "    ● Handle overfitting\n",
        "\n",
        "    ● Select base models\n",
        "\n",
        "    ● Evaluate performance using cross-validation\n",
        "\n",
        "    ● Justify how ensemble learning improves decision-making in this real-world context.\n",
        "\n",
        "    - As a data scientist tasked with predicting loan default, I would start by choosing between Bagging and Boosting based on the characteristics of the data and the problem. Since financial data often benefits from reducing bias and capturing complex patterns, I would lean towards Boosting methods like XGBoost or LightGBM, which sequentially improve on errors and often yield higher accuracy. To handle overfitting, I would apply regularization techniques such as limiting tree depth, using a low learning rate, early stopping during training, and incorporating subsampling of data or features to add randomness. For base models, decision trees are typically the preferred choice due to their ability to model nonlinear relationships and interactions within demographic and transaction features. To rigorously evaluate performance and tune hyperparameters, I would employ stratified k-fold cross-validation, ensuring that the model generalizes well across different subsets of data, and use metrics such as ROC-AUC and F1-score to account for class imbalance. Ensemble learning improves decision-making in this context by combining multiple models to reduce variance and bias, leading to more accurate and stable predictions of loan default risk. This increased accuracy helps the financial institution better assess credit risk, minimize loan losses, and make informed lending decisions, ultimately balancing profitability with risk management."
      ],
      "metadata": {
        "id": "fQaSpxiTfa4R"
      }
    }
  ]
}