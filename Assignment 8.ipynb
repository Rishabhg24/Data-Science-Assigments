{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Statistics Advance Part 1"
      ],
      "metadata": {
        "id": "kOIP8hqdLTMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a random variable in probability theory?\n",
        "   - In probability theory, a random variable is a variable that takes on different numerical values, each associated with a probability, depending on the outcome of a random experiment."
      ],
      "metadata": {
        "id": "VBiRQK1iLjSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the types of random variables?\n",
        "   - There are two main types of random variables in probability theory:\n",
        "      \n",
        "      * Discrete Random Variable\n",
        "Definition: Takes on a finite or countably infinite number of distinct values.\n",
        "           \n",
        "     Examples:\n",
        "\n",
        "           Number of heads in 5 coin tosses.\n",
        "           Number of students absent in a class.\n",
        "           Probability Distribution: Defined using a probability mass function (PMF).\n",
        "            E.g., (𝑋=𝑥)P(X=x)\n",
        "\n",
        "      * Continuous Random Variable\n",
        "Definition: Takes on uncountably infinite values, typically real numbers within an interval.\n",
        "\n",
        "    Examples:\n",
        "           Height of students in a school.\n",
        "           Time taken to run a race.\n",
        "       Probability Distribution: Defined using a probability density function (PDF).\n",
        "\n",
        "       Probability that\n",
        "𝑋\n",
        "X lies in an interval is given by:\n",
        "\n",
        "            𝑃 ( 𝑎 ≤ 𝑋 ≤ 𝑏 ) = ∫ 𝑎 𝑏 𝑓 ( 𝑥 )   𝑑 𝑥 P(a≤X≤b)=∫ a b​f(x)dx\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XymJr48JLzp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between discrete and continuous distributions?\n",
        "   - The key difference between discrete and continuous distributions lies in the type of values the random variable can take and how probabilities are assigned. A discrete distribution deals with countable outcomes, such as integers or whole numbers. It assigns probabilities to specific individual values using a probability mass function (PMF). For example, the number of heads when tossing a coin three times is a discrete variable with possible values like 0, 1, 2, or 3. On the other hand, a continuous distribution involves uncountably infinite outcomes, usually real numbers within a range, like height, weight, or time. It uses a probability density function (PDF), and probabilities are calculated over intervals rather than exact values. In fact, the probability of a continuous random variable taking on any exact value is zero. Instead, we compute the probability that the variable falls within a certain range. Thus, while discrete distributions sum individual probabilities, continuous distributions integrate over intervals.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VVkJSGm_OEHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are probability distribution functions (PDF)?\n",
        "   - A Probability Distribution Function (PDF) describes how the values of a random variable are distributed in terms of their probabilities. It tells us the likelihood of a random variable falling within a particular range."
      ],
      "metadata": {
        "id": "szez3KEjOY5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do cumulative distribution functions (CDF) differ from probability distribution functions (PDF)?\n",
        "   - The Cumulative Distribution Function (CDF) and the Probability Distribution Function (PDF) differ in what they represent and how they describe probabilities. The PDF (used for continuous variables) or PMF (for discrete variables) gives the likelihood or probability at a specific value. In contrast, the CDF gives the total probability that a random variable is less than or equal to a certain value. For a continuous random variable, the PDF is the derivative of the CDF, and the CDF is the integral of the PDF. While the PDF describes the shape of the distribution, the CDF describes how probability accumulates across values. The PDF is used to calculate probabilities over intervals, whereas the CDF directly provides cumulative probabilities. Also, PDF values can be greater than 1 (as densities), but the CDF always ranges from 0 to 1 and is non-decreasing.\n",
        "    "
      ],
      "metadata": {
        "id": "9AXooIcwOlAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is a discrete uniform distribution?\n",
        "   - A discrete uniform distribution is a type of probability distribution where all possible outcomes are equally likely. It is defined over a finite set of discrete values, and each value has the same constant probability."
      ],
      "metadata": {
        "id": "HqbtGTN_O9Pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What are the key properties of a Bernoulli distribution?\n",
        "   - The Bernoulli distribution is a discrete probability distribution for a random variable that takes only two possible outcomes: 1 (success) with probability\n",
        "𝑝\n",
        "p, and 0 (failure) with probability\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "1−p, where\n",
        "0\n",
        "≤\n",
        "𝑝\n",
        "≤\n",
        "1\n",
        "0≤p≤1. Its key properties include:\n",
        "\n",
        "* Support:\n",
        "𝑋\n",
        "∈\n",
        "{\n",
        "0\n",
        ",\n",
        "1\n",
        "}\n",
        "X∈{0,1}\n",
        "\n",
        "* Mean (Expected Value):\n",
        "𝐸\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "𝑝\n",
        "E(X)=p\n",
        "\n",
        "* Variance:\n",
        "Var\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "𝑝\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "Var(X)=p(1−p)\n",
        "\n",
        "* Probability Mass Function (PMF):\n",
        "𝑃 ( 𝑋 = 𝑥 ) = 𝑝 𝑥 ( 1 − 𝑝 ) 1 − 𝑥 , 𝑥 = 0 or 1 P(X=x)=p x (1−p) 1−x ,x=0 or 1\n",
        "* Skewness:\n",
        "1\n",
        "−\n",
        "2\n",
        "𝑝\n",
        "𝑝\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "p(1−p)1−2p\n",
        "​\n",
        "\n",
        "\n",
        "* Kurtosis (Excess):\n",
        "1\n",
        "−\n",
        "6\n",
        "𝑝\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "𝑝\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "p(1−p)\n",
        "1−6p(1−p)\n",
        "​\n",
        "\n",
        "\n",
        "The Bernoulli distribution is commonly used to model yes/no or success/failure experiments such as flipping a coin or passing/failing a test."
      ],
      "metadata": {
        "id": "Zqq0GY1-PIis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the binomial distribution, and how is it used in probability?\n",
        "   - The binomial distribution is a fundamental probability distribution used to model situations where there are a fixed number of independent trials, each with only two possible outcomes: success or failure. Each trial has the same probability of success, denoted by\n",
        "𝑝\n",
        "p, and the distribution describes the probability of getting exactly\n",
        "𝑘\n",
        "k successes in\n",
        "𝑛\n",
        "n trials. It is commonly used in scenarios such as coin tossing, quality control, clinical trials, and survey analysis—where you want to know the likelihood of a certain number of successes occurring. The binomial distribution helps in calculating probabilities, determining expected outcomes (mean =\n",
        "𝑛\n",
        "𝑝\n",
        "np), and measuring variability (variance =\n",
        "𝑛\n",
        "𝑝\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "np(1−p)) in repeated experiments or processes that follow a yes/no format."
      ],
      "metadata": {
        "id": "EfY9qehZQKJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the Poisson distribution and where is it applied?\n",
        "   - The Poisson distribution is a discrete probability distribution used to model the number of times an event occurs within a fixed interval of time, distance, area, or volume, when those events happen independently and at a constant average rate. It is particularly useful for counting the occurrence of rare or random events. The distribution is defined by a single parameter\n",
        "𝜆\n",
        "λ, which represents the average number of occurrences in the interval. It assumes that two events cannot occur at exactly the same instant and that the likelihood of an event occurring is proportional to the size of the interval. The Poisson distribution is widely applied in fields such as telecommunications (e.g., modeling call arrivals), traffic flow (e.g., number of car accidents at an intersection), biology (e.g., number of mutations in DNA), and business (e.g., customer arrivals at a service center). Its simplicity and suitability for real-world scenarios involving rare or random events make it a valuable tool in probability and statistics."
      ],
      "metadata": {
        "id": "HCi1pSDEQeZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is a continuous uniform distribution?\n",
        "    - A continuous uniform distribution is a probability distribution where a continuous random variable is equally likely to take any value within a specific interval\n",
        "[\n",
        "𝑎\n",
        ",\n",
        "𝑏\n",
        "]\n",
        "[a,b]. In this distribution, the probability is uniformly spread across the entire interval, meaning no value within the range is more likely than another. The probability density function (PDF) is constant and given by\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "𝑏\n",
        "−\n",
        "𝑎\n",
        "f(x)=\n",
        "b−a\n",
        "1\n",
        "​\n",
        "  for all\n",
        "𝑥\n",
        "x between\n",
        "𝑎\n",
        "a and\n",
        "𝑏\n",
        "b. Outside this interval, the probability is zero. The mean (expected value) of the distribution is + 𝑏 2 2 a+b​, and the variance is ( 𝑏 − 𝑎 ) 2 12 12 (b−a) 2\n",
        "​\n",
        " . This distribution is commonly used in scenarios where outcomes are equally likely across a range, such as randomly selecting a time within a given hour or choosing a point at random along a line segment. It is simple yet important in modeling and simulations when no outcome within the range is preferred over another."
      ],
      "metadata": {
        "id": "nysQ9RSJQxof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are the characteristics of a normal distribution?\n",
        "    - The normal distribution, also known as the Gaussian distribution, is a continuous probability distribution that is widely used in statistics and natural sciences due to its well-defined mathematical properties and frequent appearance in real-world data. Its key characteristics include:\n",
        "\n",
        "      * Bell-Shaped Curve: The graph is symmetric and bell-shaped, centered around the mean.\n",
        "\n",
        "      * Symmetry: It is perfectly symmetric about the mean (μ). This means the mean, median, and mode are all equal.\n",
        "     \n",
        "      * Defined by Two Parameters:\n",
        "          * Mean (μ): Determines the center or location of the curve.\n",
        "          * Standard deviation (σ): Measures the spread or width of the distribution.\n",
        "      \n",
        "      * 68-95-99.7 Rule:\n",
        "          * About 68% of the data lies within ±1σ of the mean.\n",
        "          * 95% within ±2σ.\n",
        "          * 99.7% within ±3σ.\n",
        "          \n",
        "      * Total Area Under the Curve: Equals 1, representing total probability.\n",
        "\n",
        "      * Tails Extend Infinitely: The curve never touches the x-axis, but it approaches it asymptotically.\n",
        "      \n",
        "      * Unimodal: It has only one peak, which occurs at the mean.\n",
        "      * No Skewness: Skewness is 0, indicating perfect symmetry.\n",
        "\n",
        "     The normal distribution is commonly used in quality control, standardized testing, finance, and natural phenomena like heights, weights, and measurement errors."
      ],
      "metadata": {
        "id": "Li0cDgKgR6C0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the standard normal distribution, and why is it important?\n",
        "    - The standard normal distribution is a special case of the normal distribution where the mean is 0 and the standard deviation is 1. It is a symmetrical, bell-shaped curve centered at zero, and it follows the same properties as any normal distribution, such as the 68-95-99.7 rule.\n",
        "    \n",
        "      The standard normal distribution is important because:-\n",
        "       \n",
        "       it allows for simplified calculation of probabilities and is used as a reference for comparing different normal distributions. By converting any normal distribution to the standard normal distribution using z-scores (which measure how many standard deviations a value is from the mean), we can use a common table of values—called the z-table—to find probabilities and percentiles. This process, known as standardization, is essential in hypothesis testing, confidence interval estimation, and many statistical analyses across fields such as psychology, economics, and engineering."
      ],
      "metadata": {
        "id": "rwoX3l4xTKQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is the Central Limit Theorem (CLT), and why is it critical in statistics?\n",
        "    - The Central Limit Theorem (CLT) is a fundamental concept in statistics that states: regardless of the original distribution of a population, the distribution of the sample means will tend to become approximately normal (bell-shaped) as the sample size increases, provided the samples are independent and identically distributed. This approximation to normality becomes more accurate as the sample size grows, typically becoming reliable when the sample size is 30 or more.\n",
        "\n",
        "      The CLT is critical in statistics because it justifies the use of the normal distribution in many real-world problems, even when the population itself is not normally distributed. It enables statisticians to make valid inferences—such as calculating confidence intervals and conducting hypothesis tests—using sample data. Thanks to the CLT, tools that rely on the normal distribution (like z-scores and t-tests) can be applied broadly, making it the cornerstone of much of inferential statistics."
      ],
      "metadata": {
        "id": "xHTXwPaOTpnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How does the Central Limit Theorem relate to the normal distribution?\n",
        "    - The Central Limit Theorem (CLT) is directly related to the normal distribution because it explains why and how normal distributions appear so frequently in statistics, even when the underlying population is not normally distributed. According to the CLT, if you take many random samples of a given size from any population with a finite mean and variance, the distribution of the sample means will approach a normal distribution as the sample size increases—typically when the sample size is 30 or more.\n",
        "\n",
        "     This relationship allows statisticians to apply the normal distribution as an approximation for analyzing sample means, regardless of the original data's distribution. It is this connection that makes the normal distribution a powerful tool in hypothesis testing, confidence interval estimation, and other statistical inference techniques."
      ],
      "metadata": {
        "id": "8mnBDQmxT3JK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is the application of Z statistics in hypothesis testing?\n",
        "    - Z-statistics are applied in hypothesis testing to determine whether there is a significant difference between a sample statistic and a known population parameter, under the assumption that the population standard deviation is known. In practice, the Z-statistic measures how many standard deviations the sample mean deviates from the population mean specified in the null hypothesis. It is especially useful when working with large sample sizes (typically\n",
        "𝑛\n",
        "≥\n",
        "30\n",
        "n≥30), as the sampling distribution of the mean approaches normality due to the Central Limit Theorem. The calculated Z-score is then compared to a critical value from the standard normal distribution or used to determine a p-value. If the Z-score falls in the rejection region or the p-value is below the chosen significance level (e.g., 0.05), the null hypothesis is rejected. Z-tests are commonly used in areas such as quality control, medical testing, and business analytics where the population variance is known and the goal is to test claims about population means or proportions."
      ],
      "metadata": {
        "id": "bHWusSCJUEDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  How do you calculate a Z-score, and what does it represent?\n",
        "     - A Z-score is calculated using the formula: Z = (X - μ) / σ, where X is the data point, μ is the population mean, and σ is the population standard deviation. If you are working with a sample mean, the formula becomes Z = (𝑋̄ - μ) / (σ / √n), where 𝑋̄ is the sample mean and n is the sample size. The Z-score tells you how many standard deviations a value is from the mean. A Z-score of 0 means the value is exactly at the mean, a positive Z-score means it is above the mean, and a negative Z-score means it is below the mean. Z-scores are useful for comparing values from different distributions and for finding probabilities in the standard normal distribution."
      ],
      "metadata": {
        "id": "BxS_CY6nUk8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are point estimates and interval estimates in statistics?\n",
        "    - In statistics, a point estimate is a single value used to approximate an unknown population parameter. For example, the sample mean (\n",
        "𝑥\n",
        "ˉ\n",
        "x\n",
        "ˉ\n",
        " ) is a point estimate of the population mean (\n",
        "𝜇\n",
        "μ). While point estimates give a specific value, they do not indicate how accurate that estimate is. An interval estimate, on the other hand, provides a range of values within which the population parameter is likely to fall. This is typically expressed as a confidence interval, such as “the average height is between 165 cm and 175 cm with 95% confidence.” Interval estimates give more information than point estimates because they account for sampling variability and help assess the reliability of the estimate."
      ],
      "metadata": {
        "id": "BqxNwlWXVTJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the significance of confidence intervals in statistical analysis?\n",
        "    - The significance of confidence intervals in statistical analysis lies in their ability to provide a range of plausible values for an unknown population parameter, rather than relying on a single estimate. A confidence interval reflects not only the estimate itself (like a sample mean or proportion) but also the uncertainty due to sampling variability. For example, a 95% confidence interval means that if we were to take many random samples and compute an interval for each, about 95% of those intervals would contain the true population parameter. This makes confidence intervals a powerful tool for making informed decisions, assessing the precision of estimates, and interpreting results with a clear measure of reliability. They are widely used in fields such as medicine, economics, social sciences, and engineering to support evidence-based conclusions."
      ],
      "metadata": {
        "id": "aRl3MBBBVhdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What is the relationship between a Z-score and a confidence interval?\n",
        "    - The relationship between a Z-score and a confidence interval lies in how Z-scores are used to determine the margin of error for the interval. When constructing a confidence interval for a population mean (with a known standard deviation and large sample size), the Z-score corresponds to the desired confidence level—for example:\n",
        "\n",
        "A 90% confidence level uses a Z-score of 1.645\n",
        "\n",
        "A 95% confidence level uses 1.96\n",
        "\n",
        "A 99% confidence level uses 2.576\n",
        "\n",
        "The confidence interval is calculated as:\n",
        "\n",
        "Confidence Interval\n",
        "=\n",
        "𝑋\n",
        "ˉ\n",
        "±\n",
        "𝑍\n",
        "⋅\n",
        "(\n",
        "𝜎\n",
        "𝑛\n",
        ")\n",
        "Confidence Interval=\n",
        "X\n",
        "ˉ\n",
        " ±Z⋅(\n",
        "n\n",
        "​\n",
        "\n",
        "σ\n",
        "​\n",
        " )\n",
        "Where:\n",
        "\n",
        "𝑋\n",
        "ˉ\n",
        "X\n",
        "ˉ\n",
        "  = sample mean\n",
        "\n",
        "𝜎\n",
        "σ = population standard deviation\n",
        "\n",
        "𝑛\n",
        "n = sample size\n",
        "\n",
        "𝑍\n",
        "Z = Z-score for the chosen confidence level\n",
        "\n",
        "Thus, the Z-score determines the width of the confidence interval. A higher confidence level requires a larger Z-score, which results in a wider interval, reflecting greater certainty about containing the true population parameter."
      ],
      "metadata": {
        "id": "MwxqrEFVVqzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How are Z-scores used to compare different distributions?\n",
        "    - Z-scores are used to compare different distributions by converting raw values from each distribution into a common standard scale. Since different datasets can have different means and standard deviations, directly comparing their raw values can be misleading. A Z-score measures how many standard deviations a particular value is from the mean of its distribution. By standardizing values using the formula\n",
        "𝑍\n",
        "=\n",
        "𝑋\n",
        "−\n",
        "𝜇\n",
        "𝜎\n",
        "Z=\n",
        "σ\n",
        "X−μ\n",
        "​\n",
        " , we can compare data points from different distributions regardless of their original units or scales. For example, a Z-score of 2 in one dataset and a Z-score of 1.5 in another tells us that the first value is further above its own mean than the second, even if the raw scores are not directly comparable. This makes Z-scores a useful tool in fields like education, psychology, and finance for evaluating performance, detecting outliers, and making fair comparisons across different groups or tests."
      ],
      "metadata": {
        "id": "KK8KKZbCV3ZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What are the assumptions for applying the Central Limit Theorem?\n",
        "    - The Central Limit Theorem (CLT) relies on a few key assumptions to hold true in practice. First, it assumes that the samples are independent, meaning the outcome of one observation does not influence another. Second, the sample size should be sufficiently large, typically 30 or more, especially if the population distribution is not normal. However, if the population is already normally distributed, even smaller sample sizes may be acceptable. Third, the data should come from a population with a finite mean and finite variance, as the CLT does not apply to distributions with infinite or undefined moments. When these conditions are met, the CLT states that the sampling distribution of the sample mean will approximate a normal distribution, allowing statisticians to make reliable inferences about population parameters using normal-based methods like Z-tests and confidence intervals."
      ],
      "metadata": {
        "id": "qv1t5-pRWUdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. What is the concept of expected value in a probability distribution?\n",
        "    - The expected value in a probability distribution represents the long-term average or mean outcome of a random variable if an experiment were repeated many times. It is essentially the weighted average of all possible values that a random variable can take, where each value is weighted by its probability of occurrence. For a discrete distribution, the expected value is calculated as\n",
        "𝐸\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "∑\n",
        "[\n",
        "𝑥\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "]\n",
        "E(X)=∑[x⋅P(x)], and for a continuous distribution, it is\n",
        "𝐸\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "∫\n",
        "𝑥\n",
        "⋅\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "\n",
        "𝑑\n",
        "𝑥\n",
        "E(X)=∫x⋅f(x)dx, where\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "f(x) is the probability density function. The expected value gives insight into the central tendency of the distribution and is widely used in decision-making, economics, insurance, and game theory to evaluate the average outcome of uncertain situations. While it doesn't always predict the exact outcome of a single trial, it provides a useful measure for understanding what is expected over the long run."
      ],
      "metadata": {
        "id": "fdOrGrisWp64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. How does a probability distribution relate to the expected outcome of a random variable?\n",
        "    - A probability distribution describes all the possible outcomes of a random variable along with the likelihood of each outcome occurring. It directly relates to the expected outcome—also known as the expected value—by providing the necessary probabilities to calculate a weighted average of all possible values. For a discrete random variable, the expected value is found by multiplying each possible value by its probability and summing the results:   \n",
        "       \n",
        "          𝐸 ( 𝑋 ) = ∑ [ 𝑥 ⋅ 𝑃 ( 𝑥 ) ] E(X)=∑[x⋅P(x)]\n",
        "  \n",
        "     This gives the theoretical average result if the random process were repeated many times. In essence, the probability distribution tells us how likely each outcome is, and the expected value summarizes the central or average outcome of the distribution. It is a key concept in predicting long-term behavior in uncertain situations, such as in finance, games, and risk analysis.\n",
        "  \n",
        "    "
      ],
      "metadata": {
        "id": "uU-I2RBkW2X8"
      }
    }
  ]
}